{"cells":[{"cell_type":"code","source":["import sys\nimport json\nimport pyspark\nimport pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import from_json, col, input_file_name\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["def unmount_from_adl():\n  dbutils.fs.unmount(\"/mnt/root\")\ndef mount_to_adl():\n  configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n             \"dfs.adls.oauth2.client.id\": \"1001f267-f8ee-400d-a1cb-ab70c0d97884\",\n             \"dfs.adls.oauth2.credential\": \"7hfZu9Ol/mjwshNcxIrAtrBshSMSWTP6ArJnUchRccw=\",\n             \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/6ee535f2-3064-4ac9-81d8-4ceb2ff790c6/oauth2/token\"}\n  dbutils.fs.mount(source = \"adl://maintenance40.azuredatalakestore.net/\",  mount_point = \"/mnt/root\",  extra_configs = configs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["\ndef create_paths_rawdata(year,month,day,stations = []):\n  path_to_daily_data = \"{}/{}/{}\".format(year,month,day)\n  paths =[]\n  for station in stations: \n    path = \"mnt/root/rawdata/trackcircuits/{}/bn-maintenance40-eh/trackcircuit{}/*/{}/*/*/*\".format(station, station, path_to_daily_data)\n    paths.append(path)\n  return paths\n\ndef read_rawdata(year, month, day, stations = []):\n  # Creating paths to read\n  paths = create_paths_rawdata(year = year, month = month, day = day, stations = stations)\n  # Reading data from datalake\n  return spark.read.format(\"com.databricks.spark.avro\").load(paths)\n    \ndef read_unpacked_rawdata(year, month, day, stations = []):\n  \n  df = read_rawdata(year = year, month = month, day = day, stations = stations)\n \n  json_schema = StructType([\n    StructField(\"currentDirectionKind\", StringType(), True),\n    StructField(\"eventAt\", DoubleType(), True),\n    StructField(\"measurement\", LongType(), True),\n    StructField(\"tcid\", StringType(), True)])\n    \n    #Exctracting body from binary\n  df = df.withColumn('body', df['body'].cast('string'))\n  body = df.select(\"body\")\n  data = body.withColumn('body', from_json(col('body'), json_schema)).select('body.*')\n    \n  # Adding station column (this is done by choosing the fifth element in the filepath) and this approach works for all TC not in \"SmallContributors\"-paths\n  data = data.withColumn(\"filename\", input_file_name())    \n  split_col = pyspark.sql.functions.split(data['filename'], '/')\n  data = data.withColumn('Station', split_col.getItem(5))\n  \n  #Fixing the column names (adding columns with the same name as the old data)\n  data = data.withColumn(\"orgTimestamp\", data.eventAt)\n  data = data.withColumn(\"Measurement\", data.measurement)\n  data = data.withColumn(\"syncTimestamp\", (F.round(data.eventAt/250)*250).cast(\"double\"))\n  data = data.withColumn(\"TrackCircuitId\", data.tcid)\n  data = data.withColumn(\"Current\", data.currentDirectionKind)\n  data = data.withColumn(\"Date\", F.lit(\"{}{}{}\".format(year,month,day)))\n  \n  data = data.select(\"Date\",\"Station\",\"TrackCircuitId\",\"Measurement\",\"orgTimestamp\", \"syncTimestamp\",\"Current\")\n  \n  bad_data = data.filter(\"measurement > 32000\") # A value of 32764 implies that the sensor sends out some error code and we are therefore filter these. We will have to look into it at a later period\n  good_data = data.filter(\"measurement < 32000\") # A value of 32764 implies that the sensor sends out some error code and we are therefore filter these. We will have to look into it at a later period\n\n  return good_data, bad_data\n\ndef read_preprocessed_data(folder, year, month, day, file, stations = []):\n  # Reading any of the daily preprocessed files \n  approved_files = [\"preprocessed\",\"plateau\",\"passage\"]\n  if file.lower() in approved_files:  \n    path = \"/mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/{}/\".format(year, month, day, folder, file)\n  df = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", path).load(\"{}\".format(path))\n  if stations:\n    df = df.filter(df.Station.isin(stations))\n  return df\n\ndef read_features_data(folder, year, month, day, stations = []):\n  # Reading all daily features files\n  for feature in [\"feature_measurement\", \"feature_plateau\", \"feature_passage\"]:\n    path = \"/mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/{}/\".format(year,month,day,folder,feature)\n    temp_df = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", path).load(path)\n    temp_df_pd = temp_df.filter(temp_df.Station.isin(stations)).toPandas()\n    \n    if feature == 'feature_measurement':\n      df = temp_df_pd\n    else:\n      df = pd.merge(df, temp_df_pd, on =[\"Station\",\"TrackCircuitId\"], how = \"left\")\n  \n  return df\n\ndef read_threshold_fc_data(folder, filter = []):\n  # Reading thresholddata for specific folder, can be used to only read specified trackcircuits or all track circuits if filter is not specified\n  path = '/mnt/root/ml/trackcircuits/data/threshold_fc/{}/'.format(folder)\n  if not filter: \n    df = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", path).load(path)\n  else:\n    df = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", path).load(path)\n    df = df.filter(df.TrackCircuitId.isin(filter))\n  \n  return df\n\ndef read_normalization_data(folder):\n  path = \"/mnt/root/ml/trackcircuits/data/norm_matrix/{}/\".format(folder)\n  df = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", path).load(path)\n  \n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["def perform_deadband(df, key, orderby, value_col):\n  windowSpec = \\\n    Window \\\n      .partitionBy(key) \\\n      .orderBy(orderby)\n  df = df.withColumn(\"prevval\", F.lag(df[value_col]).over(windowSpec))\n  df = df.filter(\"{} != prevval or prevval is null\".format(value_col))\n  df = df.drop(\"prevval\")\n  return df\n\n\ndef pivot_data(df, group_key, pivot_key, pivot_values, pivot_new_columns, value_col):\n  df_pivot = df.groupBy(group_key).pivot(pivot_key, pivot_values).sum(value_col)\n  i = 0\n  for value in pivot_values:\n    df_pivot = df_pivot.withColumnRenamed(value, pivot_new_columns[i])\n    i += 1\n  return df_pivot\n\ndef forward_fill(df, key, orderby, columns_to_ffill):\n  # define the window\n  windowSpec = \\\n    Window \\\n      .partitionBy(key) \\\n      .orderBy(orderby) \\\n      .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n  for column in columns_to_ffill: \n    # define the forward-filled column\n    col_ffill = F.last(df[column], ignorenulls=True).over(windowSpec)\n    # do the fill \n    df = df.withColumn(column,  col_ffill)\n  return df\n\ndef get_info_from_next_row(df, key, orderby, value_col, new_column_name):\n  windowSpec = \\\n    Window \\\n      .partitionBy(key) \\\n      .orderBy(orderby) \\\n\n  new_column = F.lead(df[value_col], 1).over(windowSpec)\n  df = df.withColumn(new_column_name, new_column)\n  return df\n\ndef get_info_from_previous_row(df, key, orderby, value_col, new_column_name):\n  windowSpec = \\\n    Window \\\n      .partitionBy(key) \\\n      .orderBy(orderby) \\\n\n  new_column = F.lag(df[value_col], 1).over(windowSpec)\n  df = df.withColumn(new_column_name, new_column)\n  return df\n\ndef get_changed_values_only(df, column_one, column_two, new_column_name, generate_sk = 0):\n  # This function checks if column_one and column_two have different values and return the value of column_two when that is true, else it returns None\n  if generate_sk == 0:\n    df = df.withColumn(new_column_name,\n                       F.when(F.isnull(df[column_two]), df[column_one])\n                       .when(df[column_two]==df[column_one], None)\n                       .otherwise(df[column_two]))\n  else: \n    df = df.withColumn(new_column_name,\n                       F.when(F.isnan(df[column_two]) ,F.monotonically_increasing_id()) # First row gets first ID\n                               .when(df[column_two]==df[column_one], None) # No id generated for rows following start of plateau\n                               .otherwise(F.monotonically_increasing_id())) # ID generated for first measurement of new plateau\n  return df\n\ndef normalize_data(df, folder):\n  norm_matrix = read_normalization_data(folder)\n  df = df.join(norm_matrix, on =[\"Station\",\"TrackCircuitId\",\"State\"], how = \"left\")\n  \n  # Calculate normalized RC and FC values as (value - mean) / stddev\n  df = df.withColumn(\"Measurement_FC_norm\", (df[\"Measurement_FC\"] - df[\"wAvgFC\"]) / df[\"wStdFC\"])\n  df = df.withColumn(\"Measurement_RC_norm\", (df[\"Measurement_RC\"] - df[\"wAvgRC\"]) / df[\"wStdRC\"])\n\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["class TrackCircuitState:\n  # Specifying states\n  TC_OCCUPIED_STATE = \"Occupied\"\n  TC_FREE_STATE = \"Free\"\n  TC_UNKNOWN_STATE = \"Unknown\"\n  TC_ARRIVING_STATE = 'Arriving'\n  TC_DEPARTING_STATE = 'Departing'\n  TC_UNCERTAIN_STATE = 'Uncertain'\n  # Setting borders for RC\n  BORDER_HIGH_RC = 140\n  BORDER_LOW_RC = 120\n  \n  def set_state_rc(self, df):\n    df = df.withColumn(\"State_RC\", \n                       F.when(df[\"Measurement_RC\"]< self.BORDER_LOW_RC, self.TC_OCCUPIED_STATE)\n                       .when(df[\"Measurement_RC\"] > self.BORDER_HIGH_RC, self.TC_FREE_STATE)\n                       .otherwise(self.TC_UNCERTAIN_STATE)\n                      )\n    return df\n    \n  def set_state_fc(self, df, folder):\n    threshold_fc = read_threshold_fc_data(folder)\n    df = df.join(threshold_fc.select(\"Station\",\"TrackCircuitId\", \"border_low_FC\", \"border_high_FC\"), [\"Station\",\"TrackCircuitId\"])\n    df = df.withColumn(\"State_FC\",\n                       F.when(df[\"Measurement_FC\"] < df[\"border_low_FC\"], self.TC_FREE_STATE)\n                       .when(df[\"Measurement_FC\"] > df[\"border_high_FC\"], self.TC_OCCUPIED_STATE)\n                       .otherwise(self.TC_UNCERTAIN_STATE))\n    return df\n  \n  def set_state_first(self, df, folder):\n    \n    df = self.set_state_rc(df)\n    df = self.set_state_fc(df, folder)\n    \n    df = df.withColumn(\"State1\",\n                       F.when(df[\"State_RC\"] == df[\"State_FC\"], df[\"State_RC\"])\n                       .otherwise(self.TC_UNKNOWN_STATE)\n                      )\n    return df\n  \n  def set_state_final(self, df, folder, calculate_first_state = 1):\n    \n    if calculate_first_state == 1:\n      #setting the first state\n      df = self.set_state_first(df, folder)\n      \n    # Checking the line of combination of states to set states Arriving and Departing\n    # In order to do that we need to attach information from last state and next state for each unknown state (and an unknown state can span for a (always unknown) number of points)\n    # previous rows\n    df = get_info_from_previous_row(df, key = [\"Station\",\"TrackCircuitId\"], orderby = \"Timestamp\", value_col = \"State1\", new_column_name = \"previousState\")\n    df = get_changed_values_only(df, column_one = \"State1\" , column_two = \"previousState\", new_column_name = 'previousPlateauState')\n    df = forward_fill(df, key = [\"Station\",\"TrackCircuitId\"], orderby = \"Timestamp\", columns_to_ffill = [\"previousPlateauState\"])\n    # next rows\n    df = get_info_from_next_row(df, key = [\"Station\",\"TrackCircuitId\"], orderby = \"Timestamp\", value_col = \"State1\", new_column_name = \"nextState\")\n    df = get_changed_values_only(df, column_one = \"State1\" , column_two = \"nextState\", new_column_name = 'nextPlateauState')\n    df = forward_fill(df, key = [\"Station\",\"TrackCircuitId\"], orderby = F.col(\"Timestamp\").desc(), columns_to_ffill = [\"nextPlateauState\"])\n    \n    # Setting final state based on the sequence of states. \n    # Final states is one of the following Free, Occupied, Unknown, Arriving, Departing\n    df = df.withColumn(\"State\", \n                       # df[\"State\"] kan only be Free, Occupied or Unknown\n                       F.when((df[\"State1\"]== self.TC_FREE_STATE) | (df[\"State1\"] == self.TC_OCCUPIED_STATE), df[\"State1\"]) \n                       # When an unknown is preceeded by an Free and succeded by an Occupied it is an arriving state\n                       .when((df[\"previousPlateauState\"] == self.TC_FREE_STATE) & (df[\"nextPlateauState\"]== self.TC_OCCUPIED_STATE), self.TC_ARRIVING_STATE) \n                       # When an unknown is preceeded by an Occupied and succeded by an Free it is an Departing state\n                       .when((df[\"previousPlateauState\"] == self.TC_OCCUPIED_STATE) & (df[\"nextPlateauState\"]== self.TC_FREE_STATE), self.TC_DEPARTING_STATE)\n                       .otherwise(self.TC_UNKNOWN_STATE) \n                              )\n    \n    # Triming the dataset, removing the first and the last line of each track circuit to avoid null-values due to lead/lag-functions\n    df = df.filter(\"nextState is not null and previousState is not null\")\n\n    df = df.select(\"Date\",\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"State\")\n    \n    return df\n  \n  #To be written\n  # calculate_fc_thresholds(self, df, folder)\n  # calculate_fc_threshold should save the thresholds\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["def wavg (values, weights):\n  wavg = F.sum(values*weights)/F.sum(weights)\n  return wavg\n\ndef wstd (values, wavg, weights): # Here we need to pass wavg as well since it dont work with a call towards function wavg\n  wv = F.sum(((values-wavg)**2)*weights) / F.sum(weights)\n  wstd_biased = F.sqrt(wv)  \n  wstd = F.when(F.sum(weights) == 1, wstd_biased).otherwise(wstd_biased * F.sum(weights) / (F.sum(weights) - 1))\n  return wstd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# This snippet is on function that returns a dataframe that describes plateaus. \n# As of now the following data will exist on plateau\n# 1. Start time, End time, Lenght and number of observations\n# 2. Minium, Maximum and weighted average of RC and FC measurement\n# 3. Weighted standard deviation of the RC and FC measuremnet\n\ndef calculate_plateau_info(df):\n  key = [\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]\n  # 1.\n  # Creating group and calculating basic information\n  tmpGrp = df.groupby(key)\n  tmpBasic = tmpGrp.agg(F.min(\"Timestamp\").alias(\"Start\"), # Min timestamp is start of plateau\n                        F.max(\"End\").alias(\"End\"), # Max end (timestamp) is end of plateau\n                        (F.max(\"End\") - F.min(\"Timestamp\")).alias(\"Length\"), # Difference between start and stop i length of plateau. protip: This should equal sum(Deltatime) F.sum(\"Deltatime\")\n                        F.count(F.lit(1)).alias(\"Count\") # Number of observations in the plateau\n                       ) \n  # 2.\n  # Calculation of min, max and weighted average\n  tmpRCFC = tmpGrp.agg(F.min(\"Measurement_RC_norm\").alias(\"Min_RC\"), # Minimum RC value\n                       F.max(\"Measurement_RC_norm\").alias(\"Max_RC\"), # Maximum RC value\n                       wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_RC\"),\n                       F.min(\"Measurement_FC_norm\").alias(\"Min_FC\"), # Minimum FC value\n                       F.max(\"Measurement_FC_norm\").alias(\"Max_FC\"), # Maximum FC value\n                       wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_FC\")\n                      )\n\n  # 3.\n  # Joining average calculation on df in order to use it in calculation for weighted standard deviation\n  df = df.join(tmpRCFC.select(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\",\"Avg_RC\", \"Avg_FC\"),\n               on = (key),\n               how = \"left\")\n  \n  # Redoing the basis for norm_matrix to include weighted average \n  tmpGrp = df.groupBy(key)\n\n  # Calculation of weighted standard deviation\n  tmpStd = tmpGrp.agg(wstd(df[\"Measurement_RC_norm\"], df[\"Avg_RC\"], df[\"DeltaWeights\"]).alias(\"Std_RC\"),\n                      wstd(df[\"Measurement_FC_norm\"], df[\"Avg_FC\"], df[\"DeltaWeights\"]).alias(\"Std_FC\")\n                     )\n\n  ########################\n  # Joining the datasets to form the df_plateau to return\n  df_plateau = tmpBasic.join(tmpRCFC, on = (key), how = \"left\") \\\n                       .join(tmpStd, on = (key), how = \"left\") \n                      \n  return df_plateau\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Creating some features on daily level based on measurements\n# E.g. min, max, std, and so on.\n\n# Creation of daily dataset (one row per TC and day that exist in the data)\n\n# 1. Unknown Ratio (both count and length)\n# 2. Free average measurements RC/FC\n# 3. Occupied average measurements RC/FC\n# 4. Free standard deviation RC/FC\n# 5. Occupied standart deviation RC/FC\n\ndef calculate_day_meas(df):\n  \n  TC_OCCUPIED_STATE = TrackCircuitState().TC_OCCUPIED_STATE\n  TC_FREE_STATE = TrackCircuitState().TC_FREE_STATE\n  TC_UNKNOWN_STATE = TrackCircuitState().TC_UNKNOWN_STATE\n  TC_ARRIVING_STATE = TrackCircuitState().TC_ARRIVING_STATE\n  TC_DEPARTING_STATE = TrackCircuitState().TC_DEPARTING_STATE\n  TC_UNCERTAIN_STATE = TrackCircuitState().TC_UNCERTAIN_STATE\n\n  # Defining a key for use in multiple places in this snippet\n  key = [\"Date\",\"Station\", \"TrackCircuitId\"]\n\n  # 1.\n\n  df_day_meas = df.groupBy(key).pivot(\"State\",[TC_FREE_STATE, TC_OCCUPIED_STATE,TC_UNKNOWN_STATE,TC_ARRIVING_STATE,TC_DEPARTING_STATE])\\\n                               .agg(F.count(F.lit(1)).alias(\"count\"),\n                                    F.sum(\"Deltatime\").alias(\"length\")\n                                   )\n\n  df_day_meas = df_day_meas.withColumn(\"FREE_COUNT\", df_day_meas[\"Free_count\"])\n  df_day_meas = df_day_meas.withColumn(\"OCCUPIED_COUNT\", df_day_meas[\"Occupied_count\"])\n  df_day_meas = df_day_meas.withColumn(\"UNKNOWN_COUNT\", df_day_meas[\"Unknown_count\"])\n  df_day_meas = df_day_meas.withColumn(\"ARRIVING_COUNT\", df_day_meas[\"Arriving_count\"])\n  df_day_meas = df_day_meas.withColumn(\"DEPARTING_COUNT\", df_day_meas[\"Departing_count\"])\n  df_day_meas = df_day_meas.withColumn(\"UNKNOWN_LENGTH\", df_day_meas[\"Unknown_length\"])\n  df_day_meas = df_day_meas.withColumn(\"ARRIVING_LENGTH\", df_day_meas[\"Arriving_length\"])\n  df_day_meas = df_day_meas.withColumn(\"DEPARTING_LENGTH\", df_day_meas[\"Departing_length\"])\n  df_day_meas = df_day_meas.withColumn(\"TOTAL_COUNT\", (F.coalesce(df_day_meas[\"FREE_COUNT\"],F.lit(0)) +\n                                                       F.coalesce(df_day_meas[\"OCCUPIED_COUNT\"],F.lit(0)) + \n                                                       F.coalesce(df_day_meas[\"UNKNOWN_COUNT\"],F.lit(0)) + \n                                                       F.coalesce(df_day_meas[\"ARRIVING_COUNT\"],F.lit(0)) +\n                                                       F.coalesce(df_day_meas[\"DEPARTING_COUNT\"],F.lit(0))))\n\n  df_day_meas = df_day_meas.withColumn(\"UNKNOWN_RATIO\", df_day_meas[\"UNKNOWN_COUNT\"]/df_day_meas[\"TOTAL_COUNT\"])\n  df_day_meas = df_day_meas.withColumn(\"ARRIVING_RATIO\", (df_day_meas[\"ARRIVING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\n  df_day_meas = df_day_meas.withColumn(\"DEPARTING_RATIO\", (df_day_meas[\"DEPARTING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\n\n  columns_included = [\"Date\",\"Station\",\"TrackCircuitId\",\"FREE_COUNT\",\"OCCUPIED_COUNT\",\"UNKNOWN_COUNT\",\"ARRIVING_COUNT\", \"DEPARTING_COUNT\", \"TOTAL_COUNT\",\"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"]\n\n  df_day_meas = df_day_meas.select(columns_included).fillna(0, subset = [\"FREE_COUNT\", \"OCCUPIED_COUNT\", \"UNKNOWN_COUNT\", \"ARRIVING_COUNT\", \n                                                                         \"DEPARTING_COUNT\", \"TOTAL_COUNT\", \"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\n                                                                         \"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"])\n\n\n  # 2.\n  tmpGrp = df.filter(\"state = '{}'\".format(TC_FREE_STATE)).groupby(key)\n  tmpFree = tmpGrp.agg(wavg(df[\"Measurement_RC_norm\"], df[\"Deltaweights\"]).alias(\"RC_F_AVG_MEASUREMENT\"),\n                       wavg(df[\"Measurement_FC_norm\"], df[\"Deltaweights\"]).alias(\"FC_F_AVG_MEASUREMENT\")\n                      )\n\n  # 3.\n  tmpGrp = df.filter(\"state = '{}'\".format(TC_OCCUPIED_STATE)).groupby(key)\n  tmpOccupied = tmpGrp.agg(wavg(df[\"Measurement_RC_norm\"], df[\"Deltaweights\"]).alias(\"RC_O_AVG_MEASUREMENT\"),\n                           wavg(df[\"Measurement_FC_norm\"], df[\"Deltaweights\"]).alias(\"FC_O_AVG_MEASUREMENT\")\n                          )\n\n  df_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\n  df_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n\n\n  # Joining average calculation on df in order to use it in calculation for weighted standard deviation\n  df = df.join(tmpFree.select(\"Date\",\"Station\",\"TrackCircuitId\",\"RC_F_AVG_MEASUREMENT\", \"FC_F_AVG_MEASUREMENT\"),\n               on = key,\n               how = \"left\")\n  df = df.join(tmpOccupied.select(\"Date\",\"Station\",\"TrackCircuitId\",\"RC_O_AVG_MEASUREMENT\", \"FC_O_AVG_MEASUREMENT\"),\n               on = key,\n               how = \"left\")\n\n\n  # Calculation of weighted standard deviation\n  # 4. \n  tmpGrp = df.filter(\"state = '{}'\".format(TC_FREE_STATE)).groupby(key)\n  tmpFree = tmpGrp.agg(wstd(df[\"Measurement_RC_norm\"], df[\"RC_F_AVG_MEASUREMENT\"], df[\"DeltaWeights\"]).alias(\"RC_F_STD_MEASUREMENT\"),\n                       wstd(df[\"Measurement_FC_norm\"], df[\"FC_F_AVG_MEASUREMENT\"], df[\"DeltaWeights\"]).alias(\"FC_F_STD_MEASUREMENT\")\n                      )\n  # 5.\n  tmpGrp = df.filter(\"state = '{}'\".format(TC_OCCUPIED_STATE)).groupby(key)\n  tmpOccupied = tmpGrp.agg(wstd(df[\"Measurement_RC_norm\"], df[\"RC_O_AVG_MEASUREMENT\"], df[\"DeltaWeights\"]).alias(\"RC_O_STD_MEASUREMENT\"),\n                           wstd(df[\"Measurement_FC_norm\"], df[\"FC_O_AVG_MEASUREMENT\"], df[\"DeltaWeights\"]).alias(\"FC_O_STD_MEASUREMENT\")\n                          )\n\n  df_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\n  df_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n  \n  return df_day_meas\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"functions","notebookId":4114650237077435},"nbformat":4,"nbformat_minor":0}
