{"cells":[{"cell_type":"code","source":["# This script will do all necessary preprocessing for daily data to be scored by AD models\n\n# 1. Reading all of one days data from datalake for choosen station\n# 2. Implement deadband-filter on data\n# 3. Create and output preprocessed dataset and related features\n# 4. Create and output plateaus and related features\n# 5. Create and output passages and related features\n\n\n# Change log\n# Date Initials Change\n# 20181004 PL Created"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# example on path to avro-file\n# /rawdata/trackcircuits/lysaker/bn-maintenance40-eh/trackcircuitlysaker/3/2018/08/29/08/02/51.avro\n# /rawdata/trackcircuits/<station>/bn-maintenance40-eh/trackciruuit<station>/<partition>/<year>/<month>/<day>/<hour>/<minute>/<AVRO-FILE>\n\n# Batch name of trained models/norm matrix\nfolder =  dbutils.widgets.get(\"folder\")\n#folder = 'sep2018_nov2018'\n\n#import datetime\n#year = datetime.datetime.today().year\n#month = datetime.datetime.today().month\n#day = datetime.datetime.today().day\n\n# Time parameters\nday =  dbutils.widgets.get(\"day\")\nmonth =  dbutils.widgets.get(\"month\")\nyear =  dbutils.widgets.get(\"year\")\n#day = '28'\n\n# Stations that should be included\nstations = [\"lysaker\", \"skoyen\", \"nationaltheatret\", \"sandvika\", \"asker\"]\n\n# doing left padding to fit format of datalake paths\nday = day.zfill(2)\nmonth = month.zfill(2)\n\npath_to_daily_data = \"{}/{}/{}\".format(year,month,day)\n\n# creating list of paths\npaths = []\nfor station in stations: \n  path = \"mnt/root/rawdata/trackcircuits/{}/bn-maintenance40-eh/trackcircuit{}/*/{}/*/*/*\".format(station, station, path_to_daily_data)\n  paths.append(path)\n\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["import sys\nimport json\nimport pyspark\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import from_json, col, input_file_name\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n\n# Reading data from datalake\ndf = spark.read.format(\"com.databricks.spark.avro\").load(paths)\n\n# Defining the schema the data is on \n## In the data lake there are more columns and some columns that are named exactly the same as the ones we want to extract but in all caps or in all none-caps. Spark is case-insensitive but by defining the JSON-schema we can get only the data we want\njson_schema = StructType([\n    StructField(\"currentDirectionKind\", StringType(), True),\n    StructField(\"eventAt\", DoubleType(), True),\n    StructField(\"measurement\", LongType(), True),\n    StructField(\"tcid\", StringType(), True)])\n\n#Exctracting body from binary                                                       \ndf = df.withColumn('body', df['body'].cast('string'))\nbody = df.select('body')\ndata = body.withColumn('body', from_json(col('body'), json_schema)).select('body.*')\n\n\n# Adding station column (this is done by choosing the fifth element in the filepath) and this approach works for all TC not in \"SmallContributors\"-paths\n\ndata = data.withColumn(\"filename\", input_file_name())\n\nsplit_col = pyspark.sql.functions.split(data['filename'], '/')\ndata = data.withColumn('Station', split_col.getItem(5))\n\ndata = data.drop(\"filename\")\nerror_code = data.filter(\"measurement > 32000\") # A value of 32764 implies that the sensor sends out some error code and we are therefore filter these. We will have to look into it at a later period\ndata = data.filter(\"measurement < 32000\") # A value of 32764 implies that the sensor sends out some error code and we are therefore filter these. We will have to look into it at a later period\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Applying deadband\n\n#print(\"Number of rows original is {}\".format(data.count()))\n\n#Fixing the column names (adding columns with the same name as the old data)\ndata = data.withColumn(\"orgTimestamp\", data.eventAt)\ndata = data.withColumn(\"Measurement\", data.measurement)\ndata = data.withColumn(\"syncTimestamp\", (F.round(data.eventAt/250)*250).cast(\"double\"))\ndata = data.withColumn(\"TrackCircuitId\", data.tcid)\ndata = data.withColumn(\"Current\", data.currentDirectionKind)\n\n# Applying a deadbandfilter. All changes in values are permitted to pass\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\", \"Current\") \\\n    .orderBy(\"orgTimestamp\")\n\ndata = data.withColumn(\"prevval\", F.lag(data.Measurement).over(windowSpec))\ndata = data.filter(\"Measurement != prevval or prevval is null\")\ndata = data.drop(\"prevval\")\n\ndata = data.select(\"Station\",\"TrackCircuitId\",\"Measurement\",\"orgTimestamp\", \"syncTimestamp\",\"Current\").cache()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Syncing time and removing duplicates\n\n# This is done by the following steps\n# 1. Timestamp is rounded to nearest 250 ms. If the value is alone on that timestamp it is chosen\n# 2. Remove duplicate values, i.e. same measurement on same rounded timestamp for same trackcircuit\n# 3. If multiple values on same rounded timestamp stil exist. The absolute differance between rounded timestamp \n#    and the original timestamp is calculated. The value with the smallest difference is selected.\n# 4. If multiple values with same differance exist. the smallest of these are selected\n\n#1 Done in previous snippets\n# data = data.withColumn(\"syncTimestamp\", (F.round(data.orgTimestamp/250)*250).cast.(\"double\"))\n#print(\"Got {} rows before syncing\".format(data.count()))\n\n#2\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\",\"Measurement\"]) # .dropDuplicates have implicit keep = first\n\n# Print that was used during development\n#print(\"Got {} rows after removing synced duplicates\".format(data.count()))\n\n# 3 # 4\ndata = data.withColumn(\"diffTimestamps\",  F.abs(data[\"orgTimestamp\"]-data[\"syncTimestamp\"]))\ndata = data.orderBy([\"diffTimestamps\", \"Measurement\"], ascending =[True,True]) # Arranging the dataframe in the order we want\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\"]) # .dropDuplicates have implicit keep = first\n\n#print(\"Got {} rows after removing synced duplicate with different measurements\".format(data.count()))\ndata = data.drop(\"diffTimestamps\")\ndata = data.withColumnRenamed('syncTimestamp','Timestamp')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Pivoting RC and FC to one row and forwardfill the values\n\n# print(\"Dataframe contains {} number of rows before pivoting\".format(df.count()))\n\n# Creating the pivoted dataframe\ndf_pivot = data.groupBy(\"Station\",\"TrackCircuitId\", \"Timestamp\").pivot(\"Current\", [\"FC\",\"RC\"]).sum(\"Measurement\")\n\n# Renaming columns\ndf_pivot = df_pivot.withColumnRenamed(\"RC\", \"Measurement_RC\")\ndf_pivot = df_pivot.withColumnRenamed(\"FC\", \"Measurement_FC\")\n#print(\"Dataframe contains {} number of rows after pivoting\".format(df_pivot.count()))\n\n\n# Forwardfilling NULL-values\n### Using window function for easy implementation\n\n# define the window\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n    .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n# define the forward-filled column\nfilled_column_RC = F.last(df_pivot['Measurement_RC'], ignorenulls=True).over(windowSpec)\nfilled_column_FC = F.last(df_pivot['Measurement_FC'], ignorenulls=True).over(windowSpec)\n\n# do the fill \ndf_pivot = df_pivot.withColumn('Measurement_RC',  filled_column_RC)\ndf_pivot = df_pivot.withColumn('Measurement_FC',  filled_column_FC)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["frequence = 4 # Data is synced to 4 hz, therefore we can say that that 1 seconds contains 4 measurements\n\n# Adding some additional information based on next row\n\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n\nendTimestamp = F.lead(df_pivot['Timestamp'],1).over(windowSpec)\n\ndf_pivot = df_pivot.withColumn(\"End\", endTimestamp)\ndf_pivot = df_pivot.withColumn(\"Deltatime\", df_pivot[\"End\"]-df_pivot[\"Timestamp\"])\ndf_pivot = df_pivot.withColumn(\"DeltatimeSeconds\", df_pivot[\"Deltatime\"]/1000) #Deltatime is given in milliseconds\ndf_pivot = df_pivot.withColumn(\"DeltaWeights\", df_pivot[\"DeltatimeSeconds\"]*frequence) #Weights for use in later calculation of average and standard deviations\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# New version, 20180918 (built ontop Mattis StreamAnalytics code) to detect wheter or not a track circuit is occupied or free\n## This could be replaced by information that is collected from Stream Analytics-calculations, however where to get these calculations are not clear today\n\n# Code used in development to start fresh\n#df_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\")\n\n# Specifying states\nTC_OCCUPIED_STATE = \"Occupied\"\nTC_FREE_STATE = \"Free\"\nTC_UNKNOWN_STATE = \"Unknown\"\nTC_ARRIVING_STATE = 'Arriving'\nTC_DEPARTING_STATE = 'Departing'\nTC_UNCERTAIN_STATE = 'Uncertain'\n\n# Setting state of RC\nborder_high_rc = 140\nborder_low_rc = 120\ndf_pivot = df_pivot.withColumn(\"State_RC\", \n                               F.when(df_pivot[\"Measurement_RC\"]< border_low_rc, TC_OCCUPIED_STATE)\n                               .when(df_pivot[\"Measurement_RC\"] > border_high_rc, TC_FREE_STATE)\n                               .otherwise(TC_UNCERTAIN_STATE)\n                              )\n\n# Setting state of FC \ntemppath = '/mnt/root/ml/trackcircuits/data/threshold_fc/{}/'.format(folder)\nthreshold_fc = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", temppath).load(temppath)\n\ndf_pivot= df_pivot.join(threshold_fc.select(\"Station\",\"TrackCircuitId\", \"border_low_FC\", \"border_high_FC\"), [\"Station\",\"TrackCircuitId\"])\ndf_pivot = df_pivot.withColumn(\"State_FC\",\n                               F.when(df_pivot[\"Measurement_FC\"] < df_pivot[\"border_low_FC\"], TC_FREE_STATE)\n                               .when(df_pivot[\"Measurement_FC\"] > df_pivot[\"border_high_FC\"], TC_OCCUPIED_STATE)\n                               .otherwise(TC_UNCERTAIN_STATE))\n\n# Filling out the first round of states (Free, Occupied, Unknown) based on RC and FC state\ndf_pivot = df_pivot.withColumn(\"State1\",\n                               F.when(df_pivot[\"State_RC\"] == df_pivot[\"State_FC\"], df_pivot[\"State_RC\"])\n                               .otherwise(TC_UNKNOWN_STATE)\n                              )\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Code to splitt up Unknown-state into Departing-, Arriving- and Unknown-states based on the track circuit behaviour before and after an unknown period\n\ndf_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"State1\")\n\n# Checking the line of combination of states to set states Arriving and Departing\n# In order to do that we need to attach information from last state and next state for each unknown state (and an unknown state can span for a (always unknown) number of points)\n\n# Adding some additional information based on next row\n\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n\nprev_state = F.lag(df_pivot['State1'],1).over(windowSpec)\nnext_state = F.lead(df_pivot[\"State1\"],1).over(windowSpec)\n\ndf_pivot = df_pivot.withColumn(\"previousState\", prev_state)\ndf_pivot = df_pivot.withColumn(\"nextState\", next_state)\n\ndf_pivot = df_pivot.withColumn(\"previousPlateauState\",\n                               F.when(F.isnull(df_pivot[\"previousState\"]), df_pivot[\"State1\"])\n                               .when(df_pivot[\"State1\"]==df_pivot[\"previousState\"], None)\n                               .otherwise(df_pivot[\"previousState\"]))\ndf_pivot = df_pivot.withColumn(\"nextPlateauState\",\n                               F.when(F.isnull(df_pivot[\"previousState\"]), df_pivot[\"State1\"])\n                               .when(df_pivot[\"State1\"]==df_pivot[\"nextState\"], None)\n                               .otherwise(df_pivot[\"nextState\"]))\n\n# Forwardfilling NULL-values\n### Using window function for easy implementation\n\n# define the window for previous state\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n    .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n# define the forward-filled column\npreviousPlataeuState = F.last(df_pivot['previousPlateauState'], ignorenulls=True).over(windowSpec)\n\n# do the fill \ndf_pivot = df_pivot.withColumn('previousPlateauState',  previousPlataeuState)\n\n\n# define the window for next state\n  # Order the data in reverse order from the previous state\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(F.col(\"Timestamp\").desc()) \\\n    .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n# define the back-filled column\nnextPlateauState = F.last(df_pivot['nextPlateauState'], ignorenulls=True).over(windowSpec)\n\n# do the fill \ndf_pivot = df_pivot.withColumn('nextPlateauState',  nextPlateauState)\n\n\n\n# Setting final state based on the sequence of states. \n# Final states is one of the following Free, Occupied, Unknown, Arriving, Departing\n\ndf_pivot = df_pivot.withColumn(\"State\", \n                               F.when((df_pivot[\"State1\"]== TC_FREE_STATE) | (df_pivot[\"State1\"] == TC_OCCUPIED_STATE), df_pivot[\"State1\"]) # df[\"State\"] kan only be Free, Occupied or Unknown\n                               .when((df_pivot[\"previousPlateauState\"] == TC_FREE_STATE) & (df_pivot[\"nextPlateauState\"]== TC_OCCUPIED_STATE), TC_ARRIVING_STATE) # When an unknown is preceeded by an Free and succeded by an Occupied it is an arriving state\n                               .when((df_pivot[\"previousPlateauState\"] == TC_OCCUPIED_STATE) & (df_pivot[\"nextPlateauState\"]== TC_FREE_STATE), TC_DEPARTING_STATE)# When an unknown is preceeded by an Occupied and succeded by an Free it is an Departing state\n                               .otherwise(TC_UNKNOWN_STATE) \n                              )\n\n\n# Triming the dataset, removing the first and the last line of each track circuit to avoid null-values due to lead/lag-functions\ndf_pivot = df_pivot.filter(\"nextState is not null and previousState is not null\")\n\ndf_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"State\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Importing the norm matrix that is to be used\n# The norm matrix is related to the data used in training of a model (i.e. a batch name) \n# There will therefore exist multiple normalization matricies (and models)\n\n\nnorm_matrix_path = \"/mnt/root/ml/trackcircuits/data/norm_matrix/{}/\".format(folder)\nnorm_matrix = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", norm_matrix_path).load(norm_matrix_path)\n\ndf_pivot = df_pivot.join(norm_matrix, on =[\"Station\",\"TrackCircuitId\",\"State\"], how = \"left\")\n\n# Calculate normalized RC and FC values as (value - mean) / stddev\ndf_pivot = df_pivot.withColumn(\"Measurement_FC_norm\", (df_pivot[\"Measurement_FC\"] - df_pivot[\"wAvgFC\"]) / df_pivot[\"wStdFC\"])\ndf_pivot = df_pivot.withColumn(\"Measurement_RC_norm\", (df_pivot[\"Measurement_RC\"] - df_pivot[\"wAvgRC\"]) / df_pivot[\"wStdRC\"])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Creating the plateaus. This is done by generating a plateauID based on matching the previous stat to current state for each track circuit\n\n# Defining a window \nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n\n# Creating a previous state\nstate = F.lag(df_pivot['State'],1).over(windowSpec)\ndf_pivot = df_pivot.withColumn(\"previousState\", state)\n\n# Generating a ID if previousstate != state for each row\ndf_pivot = df_pivot.withColumn(\"plateauId\", \n                               F.when(F.isnan(df_pivot[\"previousState\"]) ,F.monotonically_increasing_id()) # First row gets first ID\n                               .when(df_pivot[\"State\"]==df_pivot[\"PreviousState\"], None) # No id generated for rows following start of plateau\n                               .otherwise(F.monotonically_increasing_id())) # ID generated for first measurement of new plateau\n\n\n# Forwardfilling NULL-values\n### Using window function for easy implementation\n\n# define the window\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n    .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n# define the forward-filled column and filling\nplateauId = F.last(df_pivot['plateauId'], ignorenulls=True).over(windowSpec)\ndf_pivot = df_pivot.withColumn('plateauId',  plateauId)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Defining some functioned used to minimize code\n\ndef wavg (values, weights):\n  wavg = F.sum(values*weights)/F.sum(weights)\n  return wavg\n\ndef wstd (values, wavg, weights): # Here we need to pass wavg as well since it dont work with a call towards function wavg\n  wv = F.sum(((values-wavg)**2)*weights) / F.sum(weights)\n  wstd_biased = F.sqrt(wv)  \n  wstd = F.when(F.sum(weights) == 1, wstd_biased).otherwise(wstd_biased * F.sum(weights) / (F.sum(weights) - 1))\n  return wstd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Calculation of values that descirbes plateaus\n\n# 1. Min/Max/Avg for RC/FC\n# 2. Std for RC/FC\n\n# Code to start on clean snippet during development\ndf_pivot = df_pivot.select(\"TrackCircuitId\",\"State\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"wAvgFC\",\"wAvgRC\",\"wStdFC\",\"wStdRC\",\"count\",\"Measurement_FC_norm\",\"Measurement_RC_norm\",\"Station\",\"previousState\",\"plateauId\")\n\n\n# 1.\n# Creating group\ntmpGrp = df_pivot.groupby(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\")\n\ntmpBasic = tmpGrp.agg(F.min(\"Timestamp\").alias(\"Start\"), # Min timestamp is start of plateau\n                      F.max(\"End\").alias(\"End\"), # Max end (timestamp) is end of plateau\n                      (F.max(\"End\") - F.min(\"Timestamp\")).alias(\"Length\"), # Difference between start and stop i length of plateau. protip: This should equal sum(Deltatime) F.sum(\"Deltatime\")\n                      F.count(F.lit(1)).alias(\"Count\") # Number of observations in the plateau\n                     ) \n\n# Calculation of min, max and weighted average\ntmpRC = tmpGrp.agg(F.min(\"Measurement_RC_norm\").alias(\"Min_RC\"), # Minimum RC value\n                   F.max(\"Measurement_RC_norm\").alias(\"Max_RC\"), # Maximum RC value\n                   wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_RC\")\n                  )\n\ntmpFC = tmpGrp.agg(F.min(\"Measurement_FC_norm\").alias(\"Min_FC\"), # Minimum FC value\n                   F.max(\"Measurement_FC_norm\").alias(\"Max_FC\"), # Maximum FC value\n                   wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_FC\")\n                  )\n\n\n# 2.\n# Joining average calculation on df in order to use it in calculation for weighted standard deviation\ndf_pivot = df_pivot.join(tmpRC.select(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\",\"Avg_RC\"),\n                         on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]),\n                         how = \"left\")\ndf_pivot = df_pivot.join(tmpFC.select(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\",\"Avg_FC\"),\n                         on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]),\n                         how = \"left\")\n\n\n# Redoing the basis for norm_matrix to include weighted average \ntmpGrp = df_pivot.groupBy(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\")\n\n# Calculation of weighted standard deviation\ntmpStd = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Avg_RC\"], df_pivot[\"DeltaWeights\"]).alias(\"Std_RC\"),\n                    wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Avg_FC\"], df_pivot[\"DeltaWeights\"]).alias(\"Std_FC\")\n                   )\n\n\n## How are we supposed to calculate tilt in an efficient way????\n# Tilt is left out as of now but will probably return after some work\n\ndf_plateau = tmpBasic.join(tmpRC, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \\\n                     .join(tmpFC, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \\\n                     .join(tmpStd, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Creating some features on daily level based on measurements\n# E.g. min, max, std, and so on.\n\n# Creation of daily dataset (one row per TC and day that exist in the data)\n\n# 1. Unknown Ratio (both count and length)\n# 2. Free average measurements RC/FC\n# 3. Occupied average measurements RC/FC\n# 4. Free standard deviation RC/FC\n# 5. Occupied standart deviation RC/FC\n\n# Code to start snippet clean during development\ndf_pivot = df_pivot.select(\"TrackCircuitId\",\"State\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"wAvgFC\",\"wAvgRC\",\"wStdFC\",\"wStdRC\",\"count\",\"Measurement_FC_norm\",\"Measurement_RC_norm\",\"Station\",\"previousState\",\"plateauId\")\n\n# Defining a key for use in multiple places in this snippet\nkey = [\"Station\", \"TrackCircuitId\"]\n\n# 1.\n\ndf_day_meas = df_pivot.groupBy(key).pivot(\"State\",[TC_FREE_STATE, TC_OCCUPIED_STATE,TC_UNKNOWN_STATE,TC_ARRIVING_STATE,TC_DEPARTING_STATE])\\\n                                   .agg(F.count(F.lit(1)).alias(\"count\"),\n                                        F.sum(\"Deltatime\").alias(\"length\")\n                                        )\n \ndf_day_meas = df_day_meas.withColumn(\"FREE_COUNT\", df_day_meas[\"Free_count\"])\ndf_day_meas = df_day_meas.withColumn(\"OCCUPIED_COUNT\", df_day_meas[\"Occupied_count\"])\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_COUNT\", df_day_meas[\"Unknown_count\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_COUNT\", df_day_meas[\"Arriving_count\"])\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_COUNT\", df_day_meas[\"Departing_count\"])\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_LENGTH\", df_day_meas[\"Unknown_length\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_LENGTH\", df_day_meas[\"Arriving_length\"])\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_LENGTH\", df_day_meas[\"Departing_length\"])\ndf_day_meas = df_day_meas.withColumn(\"TOTAL_COUNT\", (F.coalesce(df_day_meas[\"FREE_COUNT\"],F.lit(0)) +\n                                                     F.coalesce(df_day_meas[\"OCCUPIED_COUNT\"],F.lit(0)) + \n                                                     F.coalesce(df_day_meas[\"UNKNOWN_COUNT\"],F.lit(0)) + \n                                                     F.coalesce(df_day_meas[\"ARRIVING_COUNT\"],F.lit(0)) +\n                                                     F.coalesce(df_day_meas[\"DEPARTING_COUNT\"],F.lit(0))))\n\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_RATIO\", df_day_meas[\"UNKNOWN_COUNT\"]/df_day_meas[\"TOTAL_COUNT\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_RATIO\", (df_day_meas[\"ARRIVING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_RATIO\", (df_day_meas[\"DEPARTING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\n\ncolumns_included = [\"Station\",\"TrackCircuitId\",\"FREE_COUNT\",\"OCCUPIED_COUNT\",\"UNKNOWN_COUNT\",\"ARRIVING_COUNT\", \"DEPARTING_COUNT\", \"TOTAL_COUNT\",\"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"]\n\ndf_day_meas = df_day_meas.select(columns_included).fillna(0, subset = [\"FREE_COUNT\", \"OCCUPIED_COUNT\", \"UNKNOWN_COUNT\", \"ARRIVING_COUNT\", \n                                                                       \"DEPARTING_COUNT\", \"TOTAL_COUNT\", \"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\n                                                                       \"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"])\n\n\n# 2.\ntmpGrp = df_pivot.filter(\"state = 'Free'\").groupby(key)\ntmpFree = tmpGrp.agg(wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"RC_F_AVG_MEASUREMENT\"),\n                     wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"FC_F_AVG_MEASUREMENT\")\n                    )\n\n# 3.\ntmpGrp = df_pivot.filter(\"state = 'Occupied'\").groupby(key)\ntmpOccupied = tmpGrp.agg(wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"RC_O_AVG_MEASUREMENT\"),\n                         wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"FC_O_AVG_MEASUREMENT\")\n                        )\n\ndf_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\ndf_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n\n\n# Joining average calculation on df in order to use it in calculation for weighted standard deviation\ndf_pivot = df_pivot.join(tmpFree.select(\"Station\",\"TrackCircuitId\",\"RC_F_AVG_MEASUREMENT\", \"FC_F_AVG_MEASUREMENT\"),\n                         on = key,\n                         how = \"left\")\ndf_pivot = df_pivot.join(tmpOccupied.select(\"Station\",\"TrackCircuitId\",\"RC_O_AVG_MEASUREMENT\", \"FC_O_AVG_MEASUREMENT\"),\n                         on = key,\n                         how = \"left\")\n\n\n# Calculation of weighted standard deviation\n# 4. \ntmpGrp = df_pivot.filter(\"state = 'Free'\").groupby(key)\ntmpFree = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"RC_F_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"RC_F_STD_MEASUREMENT\"),\n                     wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"FC_F_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"FC_F_STD_MEASUREMENT\")\n                    )\n# 5.\ntmpGrp = df_pivot.filter(\"state = 'Occupied'\").groupby(key)\ntmpOccupied = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"RC_O_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"RC_O_STD_MEASUREMENT\"),\n                         wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"FC_O_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"FC_O_STD_MEASUREMENT\")\n                        )\n\ndf_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\ndf_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Calculating features related to plateaus\n\nTC_OCCUPIED_STATE = \"Occupied\"\nTC_FREE_STATE = \"Free\"\n\nkey = [\"Station\",\"TrackCircuitId\"]\n\ndf_day_plat = df_plateau\\\n              .withColumn(\"diff_within_plateau_rc\",df_plateau[\"Max_RC\"] -df_plateau[\"Min_RC\"])\\\n              .withColumn(\"diff_within_plateau_fc\",df_plateau[\"Max_FC\"] -df_plateau[\"Min_FC\"])\\\n              .groupBy(key)\\\n              .pivot(\"State\", [\"Free\", \"Occupied\",\"Unknown\",\"Arriving\", \"Departing\"])\\\n              .agg(F.min(\"Std_RC\").alias(\"min_std_rc\"),\n                   F.min(\"Std_FC\").alias(\"min_std_fc\"),\n                   F.max(\"Std_RC\").alias(\"max_std_rc\"),\n                   F.max(\"Std_FC\").alias(\"max_std_fc\"),\n                   F.stddev(\"Std_RC\").alias(\"std_std_rc\"),\n                   F.stddev(\"Std_FC\").alias(\"std_std_fc\"),\n                   F.max(\"Avg_RC\").alias(\"max_avg_rc\"),\n                   F.min(\"Avg_RC\").alias(\"min_avg_rc\"),\n                   F.max(\"Avg_FC\").alias(\"max_avg_fc\"),\n                   F.min(\"Avg_FC\").alias(\"min_avg_fc\"),\n                   F.max(\"diff_within_plateau_rc\").alias(\"max_diff_max_min_rc\"),\n                   F.min(\"diff_within_plateau_rc\").alias(\"min_diff_max_min_rc\"),\n                   F.max(\"diff_within_plateau_fc\").alias(\"max_diff_max_min_fc\"),\n                   F.min(\"diff_within_plateau_fc\").alias(\"min_diff_max_min_fc\"),\n                   F.avg(\"Length\").alias(\"avg_length\"),\n                   F.max(\"Length\").alias(\"max_length\")\n                  )\n\n\n  \ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Free_max_avg_rc\"]-df_day_plat[\"Free_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Free_max_avg_fc\"]-df_day_plat[\"Free_min_avg_fc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Occupied_max_avg_rc\"]-df_day_plat[\"Occupied_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Occupied_max_avg_fc\"]-df_day_plat[\"Occupied_min_avg_fc\"])\n\n\ndf_day_plat = df_day_plat.select(\"Station\",\"TrackCircuitId\",\n                                 # Features related to STD within a plateau\n                                 \"Free_min_std_rc\", \"Free_max_std_rc\", \"Free_std_std_rc\",\n                                 \"Free_min_std_fc\", \"Free_max_std_fc\", \"Free_std_std_fc\",\n                                 \"Occupied_min_std_rc\", \"Occupied_max_std_rc\", \"Occupied_std_std_rc\",\n                                 \"Occupied_min_std_fc\", \"Occupied_max_std_fc\", \"Occupied_std_std_fc\",\n                                 # Features related to differences between plateau in avg\n                                 \"Free_diff_maxmin_avg_rc\",\"Free_diff_maxmin_avg_fc\",\n                                 \"Occupied_diff_maxmin_avg_rc\",\"Occupied_diff_maxmin_avg_fc\",\n                                 # Feature related to differences within a plateau\n                                 \"Free_max_diff_max_min_rc\", \"Free_max_diff_max_min_fc\",\n                                 \"Free_min_diff_max_min_rc\", \"Free_min_diff_max_min_fc\",\n                                 \"Occupied_max_diff_max_min_rc\", \"Occupied_max_diff_max_min_fc\",\n                                 \"Occupied_min_diff_max_min_rc\", \"Occupied_min_diff_max_min_fc\",\n                                 # Feature related to lenght of plateau\n                                 \"Arriving_avg_length\", \"Departing_avg_length\", \"Unknown_avg_length\",\n                                 \"Free_avg_length\",\"Occupied_avg_length\",\n                                 \"Arriving_max_length\", \"Departing_max_length\", \"Unknown_max_length\",\n                                 \"Free_max_length\",\"Occupied_max_length\"\n                                ).fillna(0, subset = [\"Arriving_avg_length\",\"Departing_avg_length\",\"Unknown_avg_length\",\"Free_avg_length\",\n                                                      \"Occupied_avg_length\",\n                                                      \"Arriving_max_length\",\"Departing_max_length\",\"Unknown_max_length\",\"Free_max_length\",\n                                                      \"Occupied_max_length\"])\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_rc\", \"RC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_rc\", \"RC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_rc\", \"RC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_fc\", \"FC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_fc\", \"FC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_fc\", \"FC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_rc\", \"RC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_rc\", \"RC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_rc\", \"RC_O_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_fc\", \"FC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_fc\", \"FC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_fc\", \"FC_O_STD_STD_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_rc\", \"RC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_fc\", \"FC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_rc\", \"RC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_fc\", \"FC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_rc\", \"RC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_fc\", \"FC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_rc\", \"RC_F_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_fc\", \"FC_F_MIMDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_rc\", \"RC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_fc\", \"FC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_rc\", \"RC_O_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_fc\", \"FC_O_MIMDIFF_MAX_MIN_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_avg_length\", \"ARRIVING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_avg_length\", \"DEPARTING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_avg_length\", \"UNKNOWN_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_avg_length\", \"FREE_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_avg_length\", \"OCCUPIED_AVG_LENGTH\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_max_length\", \"ARRIVING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_max_length\", \"DEPARTING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_max_length\", \"UNKNOWN_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_length\", \"FREE_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_length\", \"OCCUPIED_MAX_LENGTH\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Calculation of passages\n\ndf_passage = df_plateau.filter(\"State in ('Free','Occupied')\")\n\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Start\") \\\n\nprevstate = F.lag(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousState\", prevstate)\n\nnextstate = F.lead(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextState\", nextstate)\n\nprevAvgRC = F.lag(df_passage['Avg_RC'],1).over(windowSpec)\nprevAvgFC = F.lag(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousAvg_RC\", prevAvgRC)\ndf_passage = df_passage.withColumn(\"previousAvg_FC\", prevAvgFC)\n\nnextAvgRC = F.lead(df_passage['Avg_RC'],1).over(windowSpec)\nnextAvgFC = F.lead(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextAvg_RC\", nextAvgRC)\ndf_passage = df_passage.withColumn(\"nextAvg_FC\", nextAvgFC)\n\ndf_passage = df_passage.filter(\"State = 'Occupied' and nextState = 'Free' and previousState ='Free'\")\n\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_RC\"] -  df_passage[\"nextAvg_RC\"]))\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_FC\"] -  df_passage[\"nextAvg_FC\"]))\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_RC\"] -  df_passage[\"Avg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_FC\"] -  df_passage[\"Avg_FC\"])\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_RC\"] -  df_passage[\"nextAvg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_FC\"] -  df_passage[\"nextAvg_FC\"])\n\ndf_passage = df_passage.select(\"Station\",\"TrackCircuitId\",\n                               \"RC_PASS_AVGDIFF_BEF_AFT\",\"FC_PASS_AVGDIFF_BEF_AFT\",\n                               \"RC_PASS_AVGDIFF_BEF_PASS\",\"FC_PASS_AVGDIFF_BEF_PASS\",\n                               \"RC_PASS_AVGDIFF_PASS_AFT\",\"FC_PASS_AVGDIFF_PASS_AFT\"\n                              )\n\n\ndf_day_pass = df_passage\\\n              .groupBy(\"Station\",\"TrackCircuitId\")\\\n              .agg(# Maxium calulations\n                   F.max(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   # Minimum calc\n                   F.min(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   # Standard deviation calc\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_STDDIFF_AVG_PASS_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_STDDIFF_AVG_PASS_AFT\")\n                  )\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Writing data to files in order to maintain history and make it easier to back-etrack\n\n# Writing preprocessed data to AVRO-files\ncols = ['Station','TrackCircuitId','State','State_RC','State_FC','Timestamp','End','Deltatime','DeltatimeSeconds','DeltaWeights','Measurement_FC','Measurement_RC','Measurement_FC_norm','Measurement_RC_norm','wAvgFC','wAvgRC','wStdFC','wStdRC','count']\n\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/preprocessed'.format(year,month,day,folder)\ndf_pivot.select(cols).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing plateau data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/plateau'.format(year,month,day,folder)\ndf_plateau.write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/passage'.format(year,month,day,folder)\ndf_passage.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from measurement data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_measurement'.format(year,month,day,folder)\ndf_day_meas.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from plateaus data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_plateau'.format(year,month,day,folder)\ndf_day_plat.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_passage'.format(year,month,day,folder)\ndf_day_pass.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing number of error codes in data set \nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/error_codes'.format(year,month,day,folder)\nerror_code.groupBy(\"Station\").count().coalesce(1).write.format(\"csv\").option(\"Header\", True).save(writepath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17}],"metadata":{"name":"1 - daily_preprocess","notebookId":425520519552844},"nbformat":4,"nbformat_minor":0}
