{"cells":[{"cell_type":"code","source":["# This script will do all necessary preprocessing for daily data to be scored by AD models\n\n# 1. Reading all of one days data from datalake for choosen station\n# 2. Implement deadband-filter on data\n# 3. Create and output preprocessed dataset and related features\n# 4. Create and output plateaus and related features\n# 5. Create and output passages and related features\n\n\n# Change log\n# Date Initials Change\n# 20181004 PL Created"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# example on path to avro-file\n# /rawdata/trackcircuits/lysaker/bn-maintenance40-eh/trackcircuitlysaker/3/2018/08/29/08/02/51.avro\n# /rawdata/trackcircuits/<station>/bn-maintenance40-eh/trackciruuit<station>/<partition>/<year>/<month>/<day>/<hour>/<minute>/<AVRO-FILE>\n\n# Batch name of trained models/norm matrix\nfolder =  dbutils.widgets.get(\"folder\")\n#folder = 'sep2018_nov2018'\n\n# Time parameters\nday =  dbutils.widgets.get(\"day\")\nmonth =  dbutils.widgets.get(\"month\")\nyear =  dbutils.widgets.get(\"year\")\n#day = '08'\n#month = '01'\n#year = '2019'\n\n\n\n# Stations that should be included\nstations = [\"lysaker\", \"skoyen\", \"nationaltheatret\", \"sandvika\", \"asker\"]\n\n# doing left padding to fit format of datalake paths\nday = day.zfill(2)\nmonth = month.zfill(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%run \"/Smart Vedlikehold/Utvikling/Sporfelt/functions\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Read raw data (code for filtering/formating in function)\ndata, error_code = read_unpacked_rawdata(year = year, month = month, day = day, stations = stations)\n\n# Applying a deadbandfilter. All changes in values are permitted to pass\ndata = perform_deadband(data, key =[\"Station\",\"TrackCircuitId\",\"Current\"], orderby = \"orgTimestamp\", value_col = \"Measurement\")\ndata = data.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Syncing time and removing duplicates\n\n# This is done by the following steps\n# 1. Timestamp is rounded to nearest 250 ms. If the value is alone on that timestamp it is chosen --Changed to done before entering this snippet\n# 2. Remove duplicate values, i.e. same measurement on same rounded timestamp for same trackcircuit\n# 3. If multiple values on same rounded timestamp stil exist. The absolute differance between rounded timestamp \n#    and the original timestamp is calculated. The value with the smallest difference is selected.\n# 4. If multiple values with same differance exist. the smallest of these are selected\n\n#2\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\",\"Measurement\"]) # .dropDuplicates have implicit keep = first\n\n# 3 # 4\ndata = data.withColumn(\"diffTimestamps\",  F.abs(data[\"orgTimestamp\"]-data[\"syncTimestamp\"]))\ndata = data.orderBy([\"diffTimestamps\", \"Measurement\"], ascending =[True,True]) # Arranging the dataframe in the order we want\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\"]) # .dropDuplicates have implicit keep = first\n\ndata = data.drop(\"diffTimestamps\")\ndata = data.withColumnRenamed('syncTimestamp','Timestamp')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Pivoting RC and FC to one row and forwardfill the values\n\n# Creating the pivoted dataframe\ndf_pivot = pivot_data(df = data,\n                      group_key = [\"Date\",\"Station\",\"TrackCircuitId\", \"Timestamp\"],\n                      pivot_key = \"Current\",\n                      pivot_values = [\"FC\",\"RC\"],\n                      pivot_new_columns = [\"Measurement_FC\", \"Measurement_RC\"],\n                      value_col = \"Measurement\" )\n\n\n# Forwardfilling NULL-values\ndf_pivot = forward_fill(df = df_pivot,\n                        key = [\"Station\",\"TrackCircuitId\"],\n                        orderby = \"Timestamp\",\n                        columns_to_ffill = [\"Measurement_FC\", \"Measurement_RC\"])\n\n# Adding timestamp from last row\ndf_pivot = get_info_from_next_row(df = df_pivot,\n                                  key = [\"Station\",\"TrackCircuitId\"],\n                                  orderby = \"Timestamp\",\n                                  value_col = \"Timestamp\", \n                                  new_column_name = \"End\")\n\nfrequence = 4 # Data is synced to 4 hz, therefore we can say that that 1 seconds contains 4 measurements\n\ndf_pivot = df_pivot.withColumn(\"Deltatime\", df_pivot[\"End\"]-df_pivot[\"Timestamp\"])\ndf_pivot = df_pivot.withColumn(\"DeltatimeSeconds\", df_pivot[\"Deltatime\"]/1000) #Deltatime is given in milliseconds\ndf_pivot = df_pivot.withColumn(\"DeltaWeights\", df_pivot[\"DeltatimeSeconds\"]*frequence) #Weights for use in later calculation of average and standard deviations\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# New version, 20180918 (built ontop Mattis StreamAnalytics code) to detect wheter or not a track circuit is occupied or free\n## This could be replaced by information that is collected from Stream Analytics-calculations, however where to get these calculations are not clear today\n\n# Code used in development to start fresh\n#df_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\")\n\n# Specifying states (states are defined in class to always have the same in all script)\nTC_OCCUPIED_STATE = TrackCircuitState().TC_OCCUPIED_STATE\nTC_FREE_STATE = TrackCircuitState().TC_FREE_STATE\nTC_UNKNOWN_STATE = TrackCircuitState().TC_UNKNOWN_STATE\nTC_ARRIVING_STATE = TrackCircuitState().TC_ARRIVING_STATE\nTC_DEPARTING_STATE = TrackCircuitState().TC_DEPARTING_STATE\nTC_UNCERTAIN_STATE = TrackCircuitState().TC_UNCERTAIN_STATE\n\n# Setting first round of state\ndf_pivot = TrackCircuitState().set_state_first(df_pivot,folder)\n\n# Code to splitt up Unknown-state into Departing-, Arriving- and Unknown-states based on the track circuit behaviour before and after an unknown period\n\ndf_pivot = TrackCircuitState().set_state_final(df_pivot, folder, calculate_first_state = 0)\n\n# NOTE, State could be set in just one line, by changing calculate_first_state = 1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# The norm matrix is related to the data used in training of a model (i.e. a batch name) \n# There will therefore exist multiple normalization matricies (and models), seperated by folders\n\n#df_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"State\")\ndf_pivot = normalize_data(df_pivot, folder)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Creating the plateaus. This is done by generating a plateauID based on matching the previous stat to current state for each track circuit\n\ndf_pivot = get_info_from_previous_row(df_pivot, key = [\"Station\", \"TrackCircuitId\"], orderby = \"Timestamp\", value_col = \"State\", new_column_name = \"previousState\")\ndf_pivot = get_changed_values_only(df_pivot, column_one = \"State\", column_two =\"previousState\", new_column_name = \"plateauId\", generate_sk = 1)\ndf_pivot = forward_fill(df_pivot, key = [\"Station\", \"TrackCircuitId\"], orderby = \"Timestamp\", columns_to_ffill = ['plateauId'])\n\n# Calculation of values that describes plateaus\n# See functions-script for code\ndf_plateau = calculate_plateau_info(df_pivot)\n\n\n# Code to start on clean snippet during development (commented out in production)\n#df_pivot = df_pivot.select(\"Date\",\"TrackCircuitId\",\"State\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"wAvgFC\",\"wAvgRC\",\"wStdFC\",\"wStdRC\",\"count\",\"Measurement_FC_norm\",\"Measurement_RC_norm\",\"Station\",\"previousState\",\"plateauId\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Calculation of features on daily level based on measurements\n# E.g. min, max, std, and so on.\n# See functions-script for code\n\ndf_day_meas = calculate_day_meas(df_pivot)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Calculating features related to plateaus\n\nTC_OCCUPIED_STATE = \"Occupied\"\nTC_FREE_STATE = \"Free\"\n\nkey = [\"Station\",\"TrackCircuitId\"]\n\ndf_day_plat = df_plateau\\\n              .withColumn(\"diff_within_plateau_rc\",df_plateau[\"Max_RC\"] -df_plateau[\"Min_RC\"])\\\n              .withColumn(\"diff_within_plateau_fc\",df_plateau[\"Max_FC\"] -df_plateau[\"Min_FC\"])\\\n              .groupBy(key)\\\n              .pivot(\"State\", [\"Free\", \"Occupied\",\"Unknown\",\"Arriving\", \"Departing\"])\\\n              .agg(F.min(\"Std_RC\").alias(\"min_std_rc\"),\n                   F.min(\"Std_FC\").alias(\"min_std_fc\"),\n                   F.max(\"Std_RC\").alias(\"max_std_rc\"),\n                   F.max(\"Std_FC\").alias(\"max_std_fc\"),\n                   F.stddev(\"Std_RC\").alias(\"std_std_rc\"),\n                   F.stddev(\"Std_FC\").alias(\"std_std_fc\"),\n                   F.max(\"Avg_RC\").alias(\"max_avg_rc\"),\n                   F.min(\"Avg_RC\").alias(\"min_avg_rc\"),\n                   F.max(\"Avg_FC\").alias(\"max_avg_fc\"),\n                   F.min(\"Avg_FC\").alias(\"min_avg_fc\"),\n                   F.max(\"diff_within_plateau_rc\").alias(\"max_diff_max_min_rc\"),\n                   F.min(\"diff_within_plateau_rc\").alias(\"min_diff_max_min_rc\"),\n                   F.max(\"diff_within_plateau_fc\").alias(\"max_diff_max_min_fc\"),\n                   F.min(\"diff_within_plateau_fc\").alias(\"min_diff_max_min_fc\"),\n                   F.avg(\"Length\").alias(\"avg_length\"),\n                   F.max(\"Length\").alias(\"max_length\")\n                  )\n\n\n  \ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Free_max_avg_rc\"]-df_day_plat[\"Free_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Free_max_avg_fc\"]-df_day_plat[\"Free_min_avg_fc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Occupied_max_avg_rc\"]-df_day_plat[\"Occupied_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Occupied_max_avg_fc\"]-df_day_plat[\"Occupied_min_avg_fc\"])\n\n\ndf_day_plat = df_day_plat.select(\"Station\",\"TrackCircuitId\",\n                                 # Features related to STD within a plateau\n                                 \"Free_min_std_rc\", \"Free_max_std_rc\", \"Free_std_std_rc\",\n                                 \"Free_min_std_fc\", \"Free_max_std_fc\", \"Free_std_std_fc\",\n                                 \"Occupied_min_std_rc\", \"Occupied_max_std_rc\", \"Occupied_std_std_rc\",\n                                 \"Occupied_min_std_fc\", \"Occupied_max_std_fc\", \"Occupied_std_std_fc\",\n                                 # Features related to differences between plateau in avg\n                                 \"Free_diff_maxmin_avg_rc\",\"Free_diff_maxmin_avg_fc\",\n                                 \"Occupied_diff_maxmin_avg_rc\",\"Occupied_diff_maxmin_avg_fc\",\n                                 # Feature related to differences within a plateau\n                                 \"Free_max_diff_max_min_rc\", \"Free_max_diff_max_min_fc\",\n                                 \"Free_min_diff_max_min_rc\", \"Free_min_diff_max_min_fc\",\n                                 \"Occupied_max_diff_max_min_rc\", \"Occupied_max_diff_max_min_fc\",\n                                 \"Occupied_min_diff_max_min_rc\", \"Occupied_min_diff_max_min_fc\",\n                                 # Feature related to lenght of plateau\n                                 \"Arriving_avg_length\", \"Departing_avg_length\", \"Unknown_avg_length\",\n                                 \"Free_avg_length\",\"Occupied_avg_length\",\n                                 \"Arriving_max_length\", \"Departing_max_length\", \"Unknown_max_length\",\n                                 \"Free_max_length\",\"Occupied_max_length\"\n                                ).fillna(0, subset = [\"Arriving_avg_length\",\"Departing_avg_length\",\"Unknown_avg_length\",\"Free_avg_length\",\n                                                      \"Occupied_avg_length\",\n                                                      \"Arriving_max_length\",\"Departing_max_length\",\"Unknown_max_length\",\"Free_max_length\",\n                                                      \"Occupied_max_length\"])\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_rc\", \"RC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_rc\", \"RC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_rc\", \"RC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_fc\", \"FC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_fc\", \"FC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_fc\", \"FC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_rc\", \"RC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_rc\", \"RC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_rc\", \"RC_O_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_fc\", \"FC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_fc\", \"FC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_fc\", \"FC_O_STD_STD_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_rc\", \"RC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_fc\", \"FC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_rc\", \"RC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_fc\", \"FC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_rc\", \"RC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_fc\", \"FC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_rc\", \"RC_F_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_fc\", \"FC_F_MIMDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_rc\", \"RC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_fc\", \"FC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_rc\", \"RC_O_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_fc\", \"FC_O_MIMDIFF_MAX_MIN_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_avg_length\", \"ARRIVING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_avg_length\", \"DEPARTING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_avg_length\", \"UNKNOWN_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_avg_length\", \"FREE_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_avg_length\", \"OCCUPIED_AVG_LENGTH\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_max_length\", \"ARRIVING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_max_length\", \"DEPARTING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_max_length\", \"UNKNOWN_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_length\", \"FREE_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_length\", \"OCCUPIED_MAX_LENGTH\")\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Calculation of passages\n\ndf_passage = df_plateau.filter(\"State in ('Free','Occupied')\")\n\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Start\") \\\n\nprevstate = F.lag(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousState\", prevstate)\n\nnextstate = F.lead(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextState\", nextstate)\n\nprevAvgRC = F.lag(df_passage['Avg_RC'],1).over(windowSpec)\nprevAvgFC = F.lag(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousAvg_RC\", prevAvgRC)\ndf_passage = df_passage.withColumn(\"previousAvg_FC\", prevAvgFC)\n\nnextAvgRC = F.lead(df_passage['Avg_RC'],1).over(windowSpec)\nnextAvgFC = F.lead(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextAvg_RC\", nextAvgRC)\ndf_passage = df_passage.withColumn(\"nextAvg_FC\", nextAvgFC)\n\ndf_passage = df_passage.filter(\"State = 'Occupied' and nextState = 'Free' and previousState ='Free'\")\n\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_RC\"] -  df_passage[\"nextAvg_RC\"]))\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_FC\"] -  df_passage[\"nextAvg_FC\"]))\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_RC\"] -  df_passage[\"Avg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_FC\"] -  df_passage[\"Avg_FC\"])\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_RC\"] -  df_passage[\"nextAvg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_FC\"] -  df_passage[\"nextAvg_FC\"])\n\ndf_passage = df_passage.select(\"Station\",\"TrackCircuitId\",\n                               \"RC_PASS_AVGDIFF_BEF_AFT\",\"FC_PASS_AVGDIFF_BEF_AFT\",\n                               \"RC_PASS_AVGDIFF_BEF_PASS\",\"FC_PASS_AVGDIFF_BEF_PASS\",\n                               \"RC_PASS_AVGDIFF_PASS_AFT\",\"FC_PASS_AVGDIFF_PASS_AFT\"\n                              )\n\n\ndf_day_pass = df_passage\\\n              .groupBy(\"Station\",\"TrackCircuitId\")\\\n              .agg(# Maxium calulations\n                   F.max(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   # Minimum calc\n                   F.min(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   # Standard deviation calc\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_STDDIFF_AVG_PASS_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_STDDIFF_AVG_PASS_AFT\")\n                  )\n\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Writing data to files in order to maintain history and make it easier to back-etrack\n\n# Writing preprocessed data to AVRO-files\ncols = ['Station','TrackCircuitId','State','State_RC','State_FC','Timestamp','End','Deltatime','DeltatimeSeconds','DeltaWeights','Measurement_FC','Measurement_RC','Measurement_FC_norm','Measurement_RC_norm','wAvgFC','wAvgRC','wStdFC','wStdRC','count']\n\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/preprocessed'.format(year,month,day,folder)\ndf_pivot.select(cols).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing plateau data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/plateau'.format(year,month,day,folder)\ndf_plateau.write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/passage'.format(year,month,day,folder)\ndf_passage.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from measurement data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_measurement'.format(year,month,day,folder)\ndf_day_meas.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from plateaus data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_plateau'.format(year,month,day,folder)\ndf_day_plat.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_passage'.format(year,month,day,folder)\ndf_day_pass.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing number of error codes in data set \nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/error_codes'.format(year,month,day,folder)\nerror_code.groupBy(\"Station\").count().coalesce(1).write.format(\"csv\").option(\"Header\", True).save(writepath)"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"1 - daily_preprocess","notebookId":2041287239598177},"nbformat":4,"nbformat_minor":0}
