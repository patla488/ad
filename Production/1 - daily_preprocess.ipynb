{"cells":[{"cell_type":"code","source":["# This script will do all necessary preprocessing for daily data to be scored by AD models\n\n# 1. Reading all of one days data from datalake for choosen station\n# 2. Implement deadband-filter on data\n# 3. Create and output preprocessed dataset and related features\n# 4. Create and output plateaus and related features\n# 5. Create and output passages and related features\n\n\n# Change log\n# Date Initials Change\n# 20181004 PL Created"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# example on path to avro-file\n# /rawdata/trackcircuits/lysaker/bn-maintenance40-eh/trackcircuitlysaker/3/2018/08/29/08/02/51.avro\n# /rawdata/trackcircuits/<station>/bn-maintenance40-eh/trackciruuit<station>/<partition>/<year>/<month>/<day>/<hour>/<minute>/<AVRO-FILE>\n\n# Batch name of trained models/norm matrix\n#folder =  dbutils.widgets.get(\"folder\")\nfolder = 'sep2018_nov2018'\n\n# Time parameters\n#day =  dbutils.widgets.get(\"day\")\n#month =  dbutils.widgets.get(\"month\")\n#year =  dbutils.widgets.get(\"year\")\nday = '08'\nmonth = '01'\nyear = '2019'\n\n\n\n# Stations that should be included\nstations = [\"lysaker\", \"skoyen\", \"nationaltheatret\", \"sandvika\", \"asker\"]\n\n# doing left padding to fit format of datalake paths\nday = day.zfill(2)\nmonth = month.zfill(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%run \"/Smart Vedlikehold/Utvikling/Sporfelt/functions\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Read raw data (code for filtering/formating in function)\ndata, error_code = read_unpacked_rawdata(year = year, month = month, day = day, stations = stations)\n\n# Applying a deadbandfilter. All changes in values are permitted to pass\ndata = perform_deadband(data, key =[\"Station\",\"TrackCircuitId\",\"Current\"], orderby = \"orgTimestamp\", value_col = \"Measurement\")\ndata = data.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Syncing time and removing duplicates\n\n# This is done by the following steps\n# 1. Timestamp is rounded to nearest 250 ms. If the value is alone on that timestamp it is chosen --Changed to done before entering this snippet\n# 2. Remove duplicate values, i.e. same measurement on same rounded timestamp for same trackcircuit\n# 3. If multiple values on same rounded timestamp stil exist. The absolute differance between rounded timestamp \n#    and the original timestamp is calculated. The value with the smallest difference is selected.\n# 4. If multiple values with same differance exist. the smallest of these are selected\n\n#2\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\",\"Measurement\"]) # .dropDuplicates have implicit keep = first\n\n# 3 # 4\ndata = data.withColumn(\"diffTimestamps\",  F.abs(data[\"orgTimestamp\"]-data[\"syncTimestamp\"]))\ndata = data.orderBy([\"diffTimestamps\", \"Measurement\"], ascending =[True,True]) # Arranging the dataframe in the order we want\ndata = data.dropDuplicates([\"syncTimestamp\",\"Station\",\"TrackCircuitId\",\"Current\"]) # .dropDuplicates have implicit keep = first\n\ndata = data.drop(\"diffTimestamps\")\ndata = data.withColumnRenamed('syncTimestamp','Timestamp')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Pivoting RC and FC to one row and forwardfill the values\n\n# Creating the pivoted dataframe\ndf_pivot = pivot_data(df = data,\n                      group_key = [\"Station\",\"TrackCircuitId\", \"Timestamp\"],\n                      pivot_key = \"Current\",\n                      pivot_values = [\"FC\",\"RC\"],\n                      pivot_new_columns = [\"Measurement_FC\", \"Measurement_RC\"],\n                      value_col = \"Measurement\" )\n\n\n# Forwardfilling NULL-values\ndf_pivot = forward_fill(df = df_pivot,\n                        key = [\"Station\",\"TrackCircuitId\"],\n                        orderby = \"Timestamp\",\n                        columns_to_ffill = [\"Measurement_FC\", \"Measurement_RC\"])\n\n# Adding timestamp from last row\ndf_pivot = get_info_from_next_row(df = df_pivot,\n                                  key = [\"Station\",\"TrackCircuitId\"],\n                                  orderby = \"Timestamp\",\n                                  value_col = \"Timestamp\", \n                                  new_column_name = \"End\")\n\nfrequence = 4 # Data is synced to 4 hz, therefore we can say that that 1 seconds contains 4 measurements\n\ndf_pivot = df_pivot.withColumn(\"Deltatime\", df_pivot[\"End\"]-df_pivot[\"Timestamp\"])\ndf_pivot = df_pivot.withColumn(\"DeltatimeSeconds\", df_pivot[\"Deltatime\"]/1000) #Deltatime is given in milliseconds\ndf_pivot = df_pivot.withColumn(\"DeltaWeights\", df_pivot[\"DeltatimeSeconds\"]*frequence) #Weights for use in later calculation of average and standard deviations\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# New version, 20180918 (built ontop Mattis StreamAnalytics code) to detect wheter or not a track circuit is occupied or free\n## This could be replaced by information that is collected from Stream Analytics-calculations, however where to get these calculations are not clear today\n\n# Code used in development to start fresh\n#df_pivot = df_pivot.select(\"Station\",\"TrackCircuitId\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\")\n\n# Specifying states (states are defined in class to always have the same in all script)\nTC_OCCUPIED_STATE = TrackCircuitState().TC_OCCUPIED_STATE\nTC_FREE_STATE = TrackCircuitState().TC_FREE_STATE\nTC_UNKNOWN_STATE = TrackCircuitState().TC_UNKNOWN_STATE\nTC_ARRIVING_STATE = TrackCircuitState().TC_ARRIVING_STATE\nTC_DEPARTING_STATE = TrackCircuitState().TC_DEPARTING_STATE\nTC_UNCERTAIN_STATE = TrackCircuitState().TC_UNCERTAIN_STATE\n\n# Setting first round of state\ndf_pivot = TrackCircuitState().set_state_first(df_pivot,folder)\n\n# Code to splitt up Unknown-state into Departing-, Arriving- and Unknown-states based on the track circuit behaviour before and after an unknown period\n\ndf_pivot = TrackCircuitState().set_state_final(df_pivot, folder, calculate_first_state = 0)\n\n# NOTE, State could be set in just one line, by changing calculate_first_state = 1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Importing the norm matrix that is to be used\n# The norm matrix is related to the data used in training of a model (i.e. a batch name) \n# There will therefore exist multiple normalization matricies (and models)\n\n\nnorm_matrix_path = \"/mnt/root/ml/trackcircuits/data/norm_matrix/{}/\".format(folder)\nnorm_matrix = spark.read.format(\"com.databricks.spark.avro\").option(\"basePath\", norm_matrix_path).load(norm_matrix_path)\n\ndf_pivot = df_pivot.join(norm_matrix, on =[\"Station\",\"TrackCircuitId\",\"State\"], how = \"left\")\n\n# Calculate normalized RC and FC values as (value - mean) / stddev\ndf_pivot = df_pivot.withColumn(\"Measurement_FC_norm\", (df_pivot[\"Measurement_FC\"] - df_pivot[\"wAvgFC\"]) / df_pivot[\"wStdFC\"])\ndf_pivot = df_pivot.withColumn(\"Measurement_RC_norm\", (df_pivot[\"Measurement_RC\"] - df_pivot[\"wAvgRC\"]) / df_pivot[\"wStdRC\"])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Creating the plateaus. This is done by generating a plateauID based on matching the previous stat to current state for each track circuit\n\n# Defining a window \nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n\n# Creating a previous state\nstate = F.lag(df_pivot['State'],1).over(windowSpec)\ndf_pivot = df_pivot.withColumn(\"previousState\", state)\n\n# Generating a ID if previousstate != state for each row\ndf_pivot = df_pivot.withColumn(\"plateauId\", \n                               F.when(F.isnan(df_pivot[\"previousState\"]) ,F.monotonically_increasing_id()) # First row gets first ID\n                               .when(df_pivot[\"State\"]==df_pivot[\"PreviousState\"], None) # No id generated for rows following start of plateau\n                               .otherwise(F.monotonically_increasing_id())) # ID generated for first measurement of new plateau\n\n\n# Forwardfilling NULL-values\n### Using window function for easy implementation\n\n# define the window\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Timestamp\") \\\n    .rowsBetween(-sys.maxsize, 0) # sys.maxsize is utilize to start from beginning from each partition\n\n# define the forward-filled column and filling\nplateauId = F.last(df_pivot['plateauId'], ignorenulls=True).over(windowSpec)\ndf_pivot = df_pivot.withColumn('plateauId',  plateauId)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Defining some functioned used to minimize code\n\ndef wavg (values, weights):\n  wavg = F.sum(values*weights)/F.sum(weights)\n  return wavg\n\ndef wstd (values, wavg, weights): # Here we need to pass wavg as well since it dont work with a call towards function wavg\n  wv = F.sum(((values-wavg)**2)*weights) / F.sum(weights)\n  wstd_biased = F.sqrt(wv)  \n  wstd = F.when(F.sum(weights) == 1, wstd_biased).otherwise(wstd_biased * F.sum(weights) / (F.sum(weights) - 1))\n  return wstd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Calculation of values that descirbes plateaus\n\n# 1. Min/Max/Avg for RC/FC\n# 2. Std for RC/FC\n\n# Code to start on clean snippet during development\ndf_pivot = df_pivot.select(\"TrackCircuitId\",\"State\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"wAvgFC\",\"wAvgRC\",\"wStdFC\",\"wStdRC\",\"count\",\"Measurement_FC_norm\",\"Measurement_RC_norm\",\"Station\",\"previousState\",\"plateauId\")\n\n\n# 1.\n# Creating group\ntmpGrp = df_pivot.groupby(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\")\n\ntmpBasic = tmpGrp.agg(F.min(\"Timestamp\").alias(\"Start\"), # Min timestamp is start of plateau\n                      F.max(\"End\").alias(\"End\"), # Max end (timestamp) is end of plateau\n                      (F.max(\"End\") - F.min(\"Timestamp\")).alias(\"Length\"), # Difference between start and stop i length of plateau. protip: This should equal sum(Deltatime) F.sum(\"Deltatime\")\n                      F.count(F.lit(1)).alias(\"Count\") # Number of observations in the plateau\n                     ) \n\n# Calculation of min, max and weighted average\ntmpRC = tmpGrp.agg(F.min(\"Measurement_RC_norm\").alias(\"Min_RC\"), # Minimum RC value\n                   F.max(\"Measurement_RC_norm\").alias(\"Max_RC\"), # Maximum RC value\n                   wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_RC\")\n                  )\n\ntmpFC = tmpGrp.agg(F.min(\"Measurement_FC_norm\").alias(\"Min_FC\"), # Minimum FC value\n                   F.max(\"Measurement_FC_norm\").alias(\"Max_FC\"), # Maximum FC value\n                   wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"DeltaWeights\"]).alias(\"Avg_FC\")\n                  )\n\n\n# 2.\n# Joining average calculation on df in order to use it in calculation for weighted standard deviation\ndf_pivot = df_pivot.join(tmpRC.select(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\",\"Avg_RC\"),\n                         on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]),\n                         how = \"left\")\ndf_pivot = df_pivot.join(tmpFC.select(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\",\"Avg_FC\"),\n                         on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]),\n                         how = \"left\")\n\n\n# Redoing the basis for norm_matrix to include weighted average \ntmpGrp = df_pivot.groupBy(\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\")\n\n# Calculation of weighted standard deviation\ntmpStd = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Avg_RC\"], df_pivot[\"DeltaWeights\"]).alias(\"Std_RC\"),\n                    wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Avg_FC\"], df_pivot[\"DeltaWeights\"]).alias(\"Std_FC\")\n                   )\n\n\n## How are we supposed to calculate tilt in an efficient way????\n# Tilt is left out as of now but will probably return after some work\n\ndf_plateau = tmpBasic.join(tmpRC, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \\\n                     .join(tmpFC, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \\\n                     .join(tmpStd, on = ([\"Station\",\"TrackCircuitId\",\"State\",\"plateauId\"]), how = \"left\") \n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Creating some features on daily level based on measurements\n# E.g. min, max, std, and so on.\n\n# Creation of daily dataset (one row per TC and day that exist in the data)\n\n# 1. Unknown Ratio (both count and length)\n# 2. Free average measurements RC/FC\n# 3. Occupied average measurements RC/FC\n# 4. Free standard deviation RC/FC\n# 5. Occupied standart deviation RC/FC\n\n# Code to start snippet clean during development\ndf_pivot = df_pivot.select(\"TrackCircuitId\",\"State\",\"Timestamp\",\"Measurement_FC\",\"Measurement_RC\",\"End\",\"Deltatime\",\"DeltatimeSeconds\",\"DeltaWeights\",\"State_RC\",\"State_FC\",\"wAvgFC\",\"wAvgRC\",\"wStdFC\",\"wStdRC\",\"count\",\"Measurement_FC_norm\",\"Measurement_RC_norm\",\"Station\",\"previousState\",\"plateauId\")\n\n# Defining a key for use in multiple places in this snippet\nkey = [\"Station\", \"TrackCircuitId\"]\n\n# 1.\n\ndf_day_meas = df_pivot.groupBy(key).pivot(\"State\",[TC_FREE_STATE, TC_OCCUPIED_STATE,TC_UNKNOWN_STATE,TC_ARRIVING_STATE,TC_DEPARTING_STATE])\\\n                                   .agg(F.count(F.lit(1)).alias(\"count\"),\n                                        F.sum(\"Deltatime\").alias(\"length\")\n                                        )\n \ndf_day_meas = df_day_meas.withColumn(\"FREE_COUNT\", df_day_meas[\"Free_count\"])\ndf_day_meas = df_day_meas.withColumn(\"OCCUPIED_COUNT\", df_day_meas[\"Occupied_count\"])\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_COUNT\", df_day_meas[\"Unknown_count\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_COUNT\", df_day_meas[\"Arriving_count\"])\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_COUNT\", df_day_meas[\"Departing_count\"])\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_LENGTH\", df_day_meas[\"Unknown_length\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_LENGTH\", df_day_meas[\"Arriving_length\"])\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_LENGTH\", df_day_meas[\"Departing_length\"])\ndf_day_meas = df_day_meas.withColumn(\"TOTAL_COUNT\", (F.coalesce(df_day_meas[\"FREE_COUNT\"],F.lit(0)) +\n                                                     F.coalesce(df_day_meas[\"OCCUPIED_COUNT\"],F.lit(0)) + \n                                                     F.coalesce(df_day_meas[\"UNKNOWN_COUNT\"],F.lit(0)) + \n                                                     F.coalesce(df_day_meas[\"ARRIVING_COUNT\"],F.lit(0)) +\n                                                     F.coalesce(df_day_meas[\"DEPARTING_COUNT\"],F.lit(0))))\n\ndf_day_meas = df_day_meas.withColumn(\"UNKNOWN_RATIO\", df_day_meas[\"UNKNOWN_COUNT\"]/df_day_meas[\"TOTAL_COUNT\"])\ndf_day_meas = df_day_meas.withColumn(\"ARRIVING_RATIO\", (df_day_meas[\"ARRIVING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\ndf_day_meas = df_day_meas.withColumn(\"DEPARTING_RATIO\", (df_day_meas[\"DEPARTING_COUNT\"] / df_day_meas[\"TOTAL_COUNT\"]))\n\ncolumns_included = [\"Station\",\"TrackCircuitId\",\"FREE_COUNT\",\"OCCUPIED_COUNT\",\"UNKNOWN_COUNT\",\"ARRIVING_COUNT\", \"DEPARTING_COUNT\", \"TOTAL_COUNT\",\"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"]\n\ndf_day_meas = df_day_meas.select(columns_included).fillna(0, subset = [\"FREE_COUNT\", \"OCCUPIED_COUNT\", \"UNKNOWN_COUNT\", \"ARRIVING_COUNT\", \n                                                                       \"DEPARTING_COUNT\", \"TOTAL_COUNT\", \"UNKNOWN_RATIO\",\"ARRIVING_RATIO\",\"DEPARTING_RATIO\",\n                                                                       \"UNKNOWN_LENGTH\",\"ARRIVING_LENGTH\",\"DEPARTING_LENGTH\"])\n\n\n# 2.\ntmpGrp = df_pivot.filter(\"state = 'Free'\").groupby(key)\ntmpFree = tmpGrp.agg(wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"RC_F_AVG_MEASUREMENT\"),\n                     wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"FC_F_AVG_MEASUREMENT\")\n                    )\n\n# 3.\ntmpGrp = df_pivot.filter(\"state = 'Occupied'\").groupby(key)\ntmpOccupied = tmpGrp.agg(wavg(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"RC_O_AVG_MEASUREMENT\"),\n                         wavg(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"Deltaweights\"]).alias(\"FC_O_AVG_MEASUREMENT\")\n                        )\n\ndf_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\ndf_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n\n\n# Joining average calculation on df in order to use it in calculation for weighted standard deviation\ndf_pivot = df_pivot.join(tmpFree.select(\"Station\",\"TrackCircuitId\",\"RC_F_AVG_MEASUREMENT\", \"FC_F_AVG_MEASUREMENT\"),\n                         on = key,\n                         how = \"left\")\ndf_pivot = df_pivot.join(tmpOccupied.select(\"Station\",\"TrackCircuitId\",\"RC_O_AVG_MEASUREMENT\", \"FC_O_AVG_MEASUREMENT\"),\n                         on = key,\n                         how = \"left\")\n\n\n# Calculation of weighted standard deviation\n# 4. \ntmpGrp = df_pivot.filter(\"state = 'Free'\").groupby(key)\ntmpFree = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"RC_F_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"RC_F_STD_MEASUREMENT\"),\n                     wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"FC_F_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"FC_F_STD_MEASUREMENT\")\n                    )\n# 5.\ntmpGrp = df_pivot.filter(\"state = 'Occupied'\").groupby(key)\ntmpOccupied = tmpGrp.agg(wstd(df_pivot[\"Measurement_RC_norm\"], df_pivot[\"RC_O_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"RC_O_STD_MEASUREMENT\"),\n                         wstd(df_pivot[\"Measurement_FC_norm\"], df_pivot[\"FC_O_AVG_MEASUREMENT\"], df_pivot[\"DeltaWeights\"]).alias(\"FC_O_STD_MEASUREMENT\")\n                        )\n\ndf_day_meas = df_day_meas.join(tmpFree, on = key, how = \"left\")\ndf_day_meas = df_day_meas.join(tmpOccupied, on = key, how = \"left\")\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Calculating features related to plateaus\n\nTC_OCCUPIED_STATE = \"Occupied\"\nTC_FREE_STATE = \"Free\"\n\nkey = [\"Station\",\"TrackCircuitId\"]\n\ndf_day_plat = df_plateau\\\n              .withColumn(\"diff_within_plateau_rc\",df_plateau[\"Max_RC\"] -df_plateau[\"Min_RC\"])\\\n              .withColumn(\"diff_within_plateau_fc\",df_plateau[\"Max_FC\"] -df_plateau[\"Min_FC\"])\\\n              .groupBy(key)\\\n              .pivot(\"State\", [\"Free\", \"Occupied\",\"Unknown\",\"Arriving\", \"Departing\"])\\\n              .agg(F.min(\"Std_RC\").alias(\"min_std_rc\"),\n                   F.min(\"Std_FC\").alias(\"min_std_fc\"),\n                   F.max(\"Std_RC\").alias(\"max_std_rc\"),\n                   F.max(\"Std_FC\").alias(\"max_std_fc\"),\n                   F.stddev(\"Std_RC\").alias(\"std_std_rc\"),\n                   F.stddev(\"Std_FC\").alias(\"std_std_fc\"),\n                   F.max(\"Avg_RC\").alias(\"max_avg_rc\"),\n                   F.min(\"Avg_RC\").alias(\"min_avg_rc\"),\n                   F.max(\"Avg_FC\").alias(\"max_avg_fc\"),\n                   F.min(\"Avg_FC\").alias(\"min_avg_fc\"),\n                   F.max(\"diff_within_plateau_rc\").alias(\"max_diff_max_min_rc\"),\n                   F.min(\"diff_within_plateau_rc\").alias(\"min_diff_max_min_rc\"),\n                   F.max(\"diff_within_plateau_fc\").alias(\"max_diff_max_min_fc\"),\n                   F.min(\"diff_within_plateau_fc\").alias(\"min_diff_max_min_fc\"),\n                   F.avg(\"Length\").alias(\"avg_length\"),\n                   F.max(\"Length\").alias(\"max_length\")\n                  )\n\n\n  \ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Free_max_avg_rc\"]-df_day_plat[\"Free_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Free_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Free_max_avg_fc\"]-df_day_plat[\"Free_min_avg_fc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_rc\",\n                                     df_day_plat[\"Occupied_max_avg_rc\"]-df_day_plat[\"Occupied_min_avg_rc\"])\ndf_day_plat = df_day_plat.withColumn(\"Occupied_diff_maxmin_avg_fc\",\n                                     df_day_plat[\"Occupied_max_avg_fc\"]-df_day_plat[\"Occupied_min_avg_fc\"])\n\n\ndf_day_plat = df_day_plat.select(\"Station\",\"TrackCircuitId\",\n                                 # Features related to STD within a plateau\n                                 \"Free_min_std_rc\", \"Free_max_std_rc\", \"Free_std_std_rc\",\n                                 \"Free_min_std_fc\", \"Free_max_std_fc\", \"Free_std_std_fc\",\n                                 \"Occupied_min_std_rc\", \"Occupied_max_std_rc\", \"Occupied_std_std_rc\",\n                                 \"Occupied_min_std_fc\", \"Occupied_max_std_fc\", \"Occupied_std_std_fc\",\n                                 # Features related to differences between plateau in avg\n                                 \"Free_diff_maxmin_avg_rc\",\"Free_diff_maxmin_avg_fc\",\n                                 \"Occupied_diff_maxmin_avg_rc\",\"Occupied_diff_maxmin_avg_fc\",\n                                 # Feature related to differences within a plateau\n                                 \"Free_max_diff_max_min_rc\", \"Free_max_diff_max_min_fc\",\n                                 \"Free_min_diff_max_min_rc\", \"Free_min_diff_max_min_fc\",\n                                 \"Occupied_max_diff_max_min_rc\", \"Occupied_max_diff_max_min_fc\",\n                                 \"Occupied_min_diff_max_min_rc\", \"Occupied_min_diff_max_min_fc\",\n                                 # Feature related to lenght of plateau\n                                 \"Arriving_avg_length\", \"Departing_avg_length\", \"Unknown_avg_length\",\n                                 \"Free_avg_length\",\"Occupied_avg_length\",\n                                 \"Arriving_max_length\", \"Departing_max_length\", \"Unknown_max_length\",\n                                 \"Free_max_length\",\"Occupied_max_length\"\n                                ).fillna(0, subset = [\"Arriving_avg_length\",\"Departing_avg_length\",\"Unknown_avg_length\",\"Free_avg_length\",\n                                                      \"Occupied_avg_length\",\n                                                      \"Arriving_max_length\",\"Departing_max_length\",\"Unknown_max_length\",\"Free_max_length\",\n                                                      \"Occupied_max_length\"])\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_rc\", \"RC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_rc\", \"RC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_rc\", \"RC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_std_fc\", \"FC_F_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_std_fc\", \"FC_F_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_std_std_fc\", \"FC_F_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_rc\", \"RC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_rc\", \"RC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_rc\", \"RC_O_STD_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_std_fc\", \"FC_O_MIN_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_std_fc\", \"FC_O_MAX_STD_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_std_std_fc\", \"FC_O_STD_STD_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_rc\", \"RC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_diff_maxmin_avg_fc\", \"FC_F_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_rc\", \"RC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_diff_maxmin_avg_fc\", \"FC_O_DIFF_MAXAVG_MINAVG_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_rc\", \"RC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_diff_max_min_fc\", \"FC_F_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_rc\", \"RC_F_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_min_diff_max_min_fc\", \"FC_F_MIMDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_rc\", \"RC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_diff_max_min_fc\", \"FC_O_MAXDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_rc\", \"RC_O_MINDIFF_MAX_MIN_PLATEAU\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_min_diff_max_min_fc\", \"FC_O_MIMDIFF_MAX_MIN_PLATEAU\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_avg_length\", \"ARRIVING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_avg_length\", \"DEPARTING_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_avg_length\", \"UNKNOWN_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_avg_length\", \"FREE_AVG_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_avg_length\", \"OCCUPIED_AVG_LENGTH\")\n\ndf_day_plat = df_day_plat.withColumnRenamed(\"Arriving_max_length\", \"ARRIVING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Departing_max_length\", \"DEPARTING_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Unknown_max_length\", \"UNKNOWN_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Free_max_length\", \"FREE_MAX_LENGTH\")\ndf_day_plat = df_day_plat.withColumnRenamed(\"Occupied_max_length\", \"OCCUPIED_MAX_LENGTH\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Calculation of passages\n\ndf_passage = df_plateau.filter(\"State in ('Free','Occupied')\")\n\nwindowSpec = \\\n  Window \\\n    .partitionBy(\"Station\",\"TrackCircuitId\") \\\n    .orderBy(\"Start\") \\\n\nprevstate = F.lag(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousState\", prevstate)\n\nnextstate = F.lead(df_passage['State'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextState\", nextstate)\n\nprevAvgRC = F.lag(df_passage['Avg_RC'],1).over(windowSpec)\nprevAvgFC = F.lag(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"previousAvg_RC\", prevAvgRC)\ndf_passage = df_passage.withColumn(\"previousAvg_FC\", prevAvgFC)\n\nnextAvgRC = F.lead(df_passage['Avg_RC'],1).over(windowSpec)\nnextAvgFC = F.lead(df_passage['Avg_FC'],1).over(windowSpec)\ndf_passage = df_passage.withColumn(\"nextAvg_RC\", nextAvgRC)\ndf_passage = df_passage.withColumn(\"nextAvg_FC\", nextAvgFC)\n\ndf_passage = df_passage.filter(\"State = 'Occupied' and nextState = 'Free' and previousState ='Free'\")\n\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_RC\"] -  df_passage[\"nextAvg_RC\"]))\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_AFT\", F.abs(df_passage[\"previousAvg_FC\"] -  df_passage[\"nextAvg_FC\"]))\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_RC\"] -  df_passage[\"Avg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_BEF_PASS\", df_passage[\"previousAvg_FC\"] -  df_passage[\"Avg_FC\"])\ndf_passage = df_passage.withColumn(\"RC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_RC\"] -  df_passage[\"nextAvg_RC\"])\ndf_passage = df_passage.withColumn(\"FC_PASS_AVGDIFF_PASS_AFT\", df_passage[\"Avg_FC\"] -  df_passage[\"nextAvg_FC\"])\n\ndf_passage = df_passage.select(\"Station\",\"TrackCircuitId\",\n                               \"RC_PASS_AVGDIFF_BEF_AFT\",\"FC_PASS_AVGDIFF_BEF_AFT\",\n                               \"RC_PASS_AVGDIFF_BEF_PASS\",\"FC_PASS_AVGDIFF_BEF_PASS\",\n                               \"RC_PASS_AVGDIFF_PASS_AFT\",\"FC_PASS_AVGDIFF_PASS_AFT\"\n                              )\n\n\ndf_day_pass = df_passage\\\n              .groupBy(\"Station\",\"TrackCircuitId\")\\\n              .agg(# Maxium calulations\n                   F.max(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_AFT\"),\n                   F.max(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MAXDIFF_AVG_BEF_PASS\"),\n                   F.max(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   F.max(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MAXDIFF_AVG_PASS_AFT\"),\n                   # Minimum calc\n                   F.min(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_MINDIFF_AVG_BEF_AFT\"),\n                   F.min(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_MINDIFF_AVG_BEF_PASS\"),\n                   F.min(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   F.min(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_MINDIFF_AVG_PASS_AFT\"),\n                   # Standard deviation calc\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_AFT\").alias(\"RC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_AFT\").alias(\"FC_PASS_STDDIFF_AVG_BEF_AFT\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_BEF_PASS\").alias(\"RC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_BEF_PASS\").alias(\"FC_PASS_STDDIFF_AVG_BEF_PASS\"),\n                   F.stddev(\"RC_PASS_AVGDIFF_PASS_AFT\").alias(\"RC_PASS_STDDIFF_AVG_PASS_AFT\"),\n                   F.stddev(\"FC_PASS_AVGDIFF_PASS_AFT\").alias(\"FC_PASS_STDDIFF_AVG_PASS_AFT\")\n                  )\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Writing data to files in order to maintain history and make it easier to back-etrack\n\n# Writing preprocessed data to AVRO-files\ncols = ['Station','TrackCircuitId','State','State_RC','State_FC','Timestamp','End','Deltatime','DeltatimeSeconds','DeltaWeights','Measurement_FC','Measurement_RC','Measurement_FC_norm','Measurement_RC_norm','wAvgFC','wAvgRC','wStdFC','wStdRC','count']\n\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/preprocessed'.format(year,month,day,folder)\ndf_pivot.select(cols).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing plateau data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/plateau'.format(year,month,day,folder)\ndf_plateau.write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/passage'.format(year,month,day,folder)\ndf_passage.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from measurement data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_measurement'.format(year,month,day,folder)\ndf_day_meas.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from plateaus data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_plateau'.format(year,month,day,folder)\ndf_day_plat.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing feature from passages data to AVRO-files\nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/feature_passage'.format(year,month,day,folder)\ndf_day_pass.coalesce(1).write.partitionBy(\"Station\").format(\"com.databricks.spark.avro\").save(writepath)\n\n# Writing number of error codes in data set \nwritepath = 'mnt/root/ml/trackcircuits/data/daily/{}/{}/{}/{}/error_codes'.format(year,month,day,folder)\nerror_code.groupBy(\"Station\").count().coalesce(1).write.format(\"csv\").option(\"Header\", True).save(writepath)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15}],"metadata":{"name":"1 - daily_preprocess","notebookId":425520519552844},"nbformat":4,"nbformat_minor":0}
